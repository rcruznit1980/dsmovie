<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0" uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>Extracting dependencies from Python packages</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/01/14/extracting-dependencies-python-packages" /><author><name>Fridolin Pokorny</name></author><id>91fcccf3-b847-401b-89a1-6073c5d82959</id><updated>2022-01-14T07:00:00Z</updated><published>2022-01-14T07:00:00Z</published><summary type="html">&lt;p&gt;Python's easy-to-learn syntax and rich standard library, combined with the large number of &lt;a href="https://developers.redhat.com/topics/open-source"&gt;open source&lt;/a&gt; software packages available on the &lt;a href="https://pypi.org/"&gt;Python Package Index&lt;/a&gt; (PyPI), make it a common programming language of choice for quick prototyping leading to production systems. &lt;a href="https://developers.redhat.com/topics/python"&gt;Python&lt;/a&gt; is a good fit for many use cases, and is particularly popular in the &lt;a href="https://developers.redhat.com/topics/data-science"&gt;data science&lt;/a&gt; domain for data exploration and analysis.&lt;/p&gt; &lt;p&gt;Thus, Python's rapid rise on the &lt;a href="https://www.tiobe.com/tiobe-index/python/"&gt;TIOBE Index&lt;/a&gt; of the most popular programming languages shouldn't be a surprise. PyPI hosts more than 3 million releases of Python packages. Each package release has metadata associated with it, which makes the packages themselves an interesting dataset to explore and experiment with.&lt;/p&gt; &lt;p&gt;In this article, you'll learn how to extract metadata and dependency information from Python package releases. You'll also see how this process works in &lt;a href="https://thoth-station.ninja/"&gt;Project Thoth&lt;/a&gt;, which provides Python programmers with information about support for the packages they use, along with the dependencies, performance, and security of those packages.&lt;/p&gt; &lt;h2&gt;Python package releases and PyPI&lt;/h2&gt; &lt;p&gt;The bar chart in Figure 1 shows the number of Python package releases on PyPI from March 2005 to mid-July 2021, with each bar representing one month. As you can see, the number of package releases is growing more or less exponentially.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/rh-thoth-resolver-pypi_growth-fig1.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/rh-thoth-resolver-pypi_growth-fig1.png?itok=Dw6ht_LK" width="600" height="598" alt="A chart showing Python package releases available on PyPI from March 2005 until mid-July 2021. Each bar represents the number of Python package releases available per month." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1. The number of Python package releases available on PyPI from March 2005 until mid-July 2021. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;As the Python Package Index's name suggests, it really is &lt;a href="https://pretalx.com/packagingcon-2021/talk/X8N8ME/"&gt;an index of software packages&lt;/a&gt; (as an example, see the &lt;a href="https://pypi.org/simple/flask/"&gt;links for Flask releases&lt;/a&gt;). A simple artifact listing has its pros and cons. One of the advantages is easy artifact serving for self-hosted Python package indexes or mirrors. If you provide a simple HTTP server with exposed content conforming to the &lt;a href="https://www.python.org/dev/peps/pep-0503/"&gt;Simple Repository API&lt;/a&gt; (Python Enhancement Proposal 503), then all the Python client tooling, such as &lt;a href="https://github.com/pypa/pip"&gt;pip&lt;/a&gt;, will be automatically able to use your self-hosted Python package indexes and install packages from your server. A downside of this approach is the lack of additional package metadata, especially dependency information.&lt;/p&gt; &lt;h2&gt;Why collecting Python dependency information is challenging&lt;/h2&gt; &lt;p&gt;Dustin Ingram, a PyPI maintainer, wrote about the challenges of collecting Python dependency information in &lt;a href="https://dustingram.com/articles/2018/03/05/why-pypi-doesnt-know-dependencies"&gt;Why PyPI doesn't know your project's dependencies&lt;/a&gt;. In short, Python's source distributions execute code that is supposed to provide information about dependencies at installation time. Because the dependency listing is not provided statically, but is a result of arbitrary code execution, dependencies can be specific to the installation-script logic. This allows for computing dependencies at installation time and gives the power to express dependencies dynamically. On the other hand, the behavior is generally unpredictable and can cause headaches when trying to obtain dependency information for a package release.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; Dependencies are generally computed based on the runtime environment where the installation process executes arbitrary code. As a result, the installation can be used by malicious Python package releases to &lt;a href="https://developers.redhat.com/articles/2021/12/21/prevent-python-dependency-confusion-attacks-thoth"&gt;steal environment information or perform other malicious actions at installation time&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Recent changes to Python packaging standards have shifted away from providing dependency information during installation, and toward exposing it statically in built &lt;a href="https://www.python.org/dev/peps/pep-0427"&gt;wheels&lt;/a&gt; (PEP 427). Newer Python package releases often follow this trend, but Python packaging and tooling also tries to be backward compatible as much as possible. For a more in-depth explanation, see &lt;a href="https://pretalx.com/packagingcon-2021/talk/X8N8ME"&gt;Python packaging: Why don’t you just...?&lt;/a&gt;, a presentation from Tzu-ping Chung, one of the Python package maintainers.&lt;/p&gt; &lt;h2&gt;How Thoth collects dependency information&lt;/h2&gt; &lt;p&gt;Python artifacts specific to a Python package release can provide multiple builds besides source distributions. These builds target different environments and respect Python's &lt;a href="https://www.python.org/dev/peps/pep-0425"&gt;packaging tags for built distributions&lt;/a&gt; (PEP 425). It's up to pip (or whatever installer you choose) to select the correct built distribution for the environment in which the installer is running. These tags can specify ABI, platform, or other requirements for the target environment, as discussed in the PEP 425 documentation. If none of the built distributions match the target environment, the installer can fall back to installing the release from source distributions if provided. This process might involve additional requirements for the target environment, such as a compatible build toolchain if source distributions require building native extensions.&lt;/p&gt; &lt;p&gt;To streamline the whole process, &lt;a href="https://thoth-station.ninja/"&gt;Project Thoth&lt;/a&gt; offers a component that re-uses the logic that performs these actions in pip. This component, &lt;a href="https://github.com/thoth-station/solver"&gt;thoth-solver&lt;/a&gt;, is written as a Python application that is primarily designed to run in &lt;a href="https://developers.redhat.com/topics/containers"&gt;containerized&lt;/a&gt; environments. The thoth-solver component installs Python packages in the specified version from the desired Python package index, by letting pip decide which Python artifact should be installed into the environment where thoth-solver runs. This naturally can involve triggering package builds out of source distributions as necessary. Once the package is installed using pip's logic, thoth-solver extracts metadata out of the installed artifact, together with additional information about the thoth-solver run itself.&lt;/p&gt; &lt;p&gt;The result is a JSON document containing information about the artifact together with the environment in which the solver runs, Python-specific entries (such as hashes of files), and &lt;a href="https://packaging.python.org/en/latest/specifications/core-metadata/"&gt;Python's core metadata&lt;/a&gt;. It may also include additional dependency information, such as details about version range specifications, versions matching version range specifications of dependencies, extras, or environment markers, along with evaluation results specifically tailored for the containerized environment (see &lt;a href="https://www.python.org/dev/peps/pep-0508/"&gt;PEP 508 for more information&lt;/a&gt;). Thoth can obtain this information from multiple Python package indexes that host artifacts analyzed by thoth-solver as well as dependencies for artifacts hosted on other indexes (for example, &lt;a href="https://tensorflow.pypi.thoth-station.ninja/"&gt;AVX2-enabled builds of TensorFlow hosted on the AI Center of Excellence index&lt;/a&gt;). The procedure and data aggregated allow Thoth to check how packages form dependencies across different Python package indexes for cross-index Python package resolution.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; If a given package is not installable into the containerized environment (due to incompatibilities between Python 2 and 3, or a missing build toolchain, for example), thoth-solver reports information about the failure that can be further post-processed to extract relevant details and classify the error.&lt;/p&gt; &lt;p&gt;To see how thoth-solver works in practice, take a look at this &lt;a href="https://gist.github.com/fridex/48aa44a7a348f63da068a5174b48eb1b"&gt;example output from a thoth-solver run for Flask in version 2.0.2 available from PyPI&lt;/a&gt;. The result gives information about dependencies for &lt;a href="https://pypi.org/project/Flask/2.0.2/"&gt;flask==2.0.2&lt;/a&gt; when installed into a containerized &lt;a href="https://developers.redhat.com/products/rhel/ubi"&gt;Red Hat Universal Base Image&lt;/a&gt; &lt;a href="https://developers.redhat.com/products/rhel/download"&gt;Red Hat Enterprise Linux 8&lt;/a&gt; environment running Python 3.8 at the given point in time. The containerized environment is available on Quay as &lt;a href="https://quay.io/repository/thoth-station/solver-rhel-8-py38"&gt;solver-rhel-8-py38&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Using thoth-solver&lt;/h2&gt; &lt;p&gt;The thoth-solver component is part of Project Thoth's cloud-based Python resolver. It aggregates information about dependencies in &lt;a href="https://developers.redhat.com/blog/2021/04/26/continuous-learning-in-project-thoth-using-kafka-and-argo"&gt;Thoth's background data aggregation&lt;/a&gt; and makes them available for &lt;a href="https://developers.redhat.com/articles/2021/11/17/customize-python-dependency-resolution-machine-learning"&gt;Thoth's resolver&lt;/a&gt;. The Thoth team provides &lt;a href="https://github.com/thoth-station/solver/tree/master/overlays"&gt;multiple thoth-solver containerized environments&lt;/a&gt;, built container images of which are &lt;a href="https://quay.io/organization/thoth-station"&gt;available on Quay&lt;/a&gt;. These compute dependency information specifically for their &lt;em&gt;target environment&lt;/em&gt;—a reproducible environment with a predefined software stack—for each desired Python package release individually.&lt;/p&gt; &lt;p&gt;Keep in mind that the computed dependency information is specific to the particular point in time when thoth-solver is run. As packages get new releases, another component in Thoth—the &lt;a href="https://github.com/thoth-station/revsolver"&gt;revsolver&lt;/a&gt;, or "reverse solver"—can keep the dependency information up to date. The revsolver component uses data that has already been computed by thoth-solver and is available in a queryable form in Thoth's database. In this case, revsolver does not download any artifacts, but instead uses an already captured dependency graph available to propagate information about a new package release, which becomes part of the updated ecosystem's dependency graph available in the database.&lt;/p&gt; &lt;h2&gt;About Project Thoth&lt;/h2&gt; &lt;p&gt;As part of Project Thoth, we are accumulating knowledge to help Python developers create healthy applications. If you would like to follow updates, feel free to subscribe to our &lt;a href="https://www.youtube.com/channel/UClUIDuq_hQ6vlzmqM59B2Lw"&gt;YouTube channel&lt;/a&gt; or follow us on the &lt;a href="https://twitter.com/thothstation"&gt;@ThothStation&lt;/a&gt; Twitter handle.&lt;/p&gt; &lt;p&gt;To send us feedback or get involved in improving the Python ecosystem, please contact the Thoth Station &lt;a href="https://github.com/thoth-station/support"&gt;support repository&lt;/a&gt;. You can also directly reach out to the Thoth team on Twitter. You can report any issues you've spotted in open source Python libraries to the support repository or &lt;a href="https://thoth-station.ninja/docs/developers/adviser/prescription.html"&gt;directly write prescriptions for the resolver&lt;/a&gt; and send them to our &lt;a href="https://github.com/thoth-station/prescriptions/"&gt;prescriptions repository&lt;/a&gt;. By participating in these ways, you can help the Python cloud-based resolver come up with better recommendations.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/01/14/extracting-dependencies-python-packages" title="Extracting dependencies from Python packages"&gt;Extracting dependencies from Python packages&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Fridolin Pokorny</dc:creator><dc:date>2022-01-14T07:00:00Z</dc:date></entry><entry><title type="html">RESTEasy 6.0.0 Released</title><link rel="alternate" href="https://resteasy.github.io/2022/01/13/resteasy-6.0.0-release/" /><author><name /></author><id>https://resteasy.github.io/2022/01/13/resteasy-6.0.0-release/</id><updated>2022-01-13T18:11:11Z</updated><dc:creator /></entry><entry><title type="html">How to configure Web applications request limits in WildFly</title><link rel="alternate" href="http://www.mastertheboss.com/web/jboss-web-server/how-to-configure-web-applications-request-limits-in-wildfly/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=how-to-configure-web-applications-request-limits-in-wildfly" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/web/jboss-web-server/how-to-configure-web-applications-request-limits-in-wildfly/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=how-to-configure-web-applications-request-limits-in-wildfly</id><updated>2022-01-13T14:55:14Z</updated><content type="html">In this article you will learn which strategies you can adopt on WildFly application server to configure the maximum number of concurrent requests using either a programmatic approach (MicroProfile Fault Tolerance) or declarative one (Undertow configuration). Implementing a policy to define the number of concurrent requests is crucial to limit requests and prevent faults from ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title>A developer's guide to CI/CD and GitOps with Jenkins Pipelines</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/01/13/developers-guide-cicd-and-gitops-jenkins-pipelines" /><author><name>Bob Reselman</name></author><id>19d5abe2-3efb-49c1-91ad-8752f9686426</id><updated>2022-01-13T07:00:00Z</updated><published>2022-01-13T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/topics/ci-cd"&gt;CI/CD&lt;/a&gt;, or continuous integration and continuous delivery, is an essential part of the modern software development life cycle. Coupled with &lt;a href="https://developers.redhat.com/topics/gitops"&gt;GitOps&lt;/a&gt;, CI/CD allows developers to release high-quality software almost as soon as they commit code to a repository such as GitHub.&lt;/p&gt; &lt;p&gt;Automation is a key factor for implementing effective CI/CD. In this process, developers and release engineers create scripts that have all the instructions needed to test the code in a source code repository before putting it into a production environment. The process is efficient but complex. Fortunately, there are many tools that lessen the burden.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.jenkins.io"&gt;Jenkins&lt;/a&gt; is one of the most popular tools used for CI/CD. Jenkins has been around for years and has undergone numerous revisions, adding features all along the way. One of the most transformative features added to Jenkins is the ability to run &lt;a href="https://www.jenkins.io/doc/book/pipeline/"&gt;Jenkins Pipeline jobs&lt;/a&gt; driven by an automation script stored in a Jenkinsfile. Developers and release engineers can use Jenkinsfiles to combine the practices of CI/CD and GitOps into a unified deployment process. That's the focus of this article.&lt;/p&gt; &lt;p&gt;We'll start with a brief refresher of what Jenkins is and how it applies to both CI/CD and GitOps. Then, I’ll guide you through how to use a Jenkinsfile to create deployments that combine CI/CD and GitOps.&lt;/p&gt; &lt;h2&gt;How Jenkins supports CI/CD&lt;/h2&gt; &lt;p&gt;Jenkins is an open source tool for managing deployment processes, which can range from a single task—such as running a unit test against source code—to a complex deployment process embodying many tasks. From its first release, Jenkins allowed companies to standardize their deployment process: Once a job was configured on the Jenkins server, that job could run repeatedly in the same manner according to its configuration. The developer defined the tasks to run and when to run them, and Jenkins did the rest.&lt;/p&gt; &lt;p&gt;Early releases of Jenkins required developers to define their deployment processes manually, using the Jenkins Dashboard. Moreover, each job was specific to the particular Jenkins server. Deployments (&lt;em&gt;aka&lt;/em&gt; jobs) were not easy to update or transfer among servers. If developers wanted to update a particular job, they had to go to the server’s Jenkins Dashboard and manually implement the update. And, if developers or sysadmins wanted to move a job to another Jenkins server, they had to get into the file system of the Jenkins server and copy particular directories to other target Jenkins servers. The process was laborious, particularly if the job in question was large and contained many details.&lt;/p&gt; &lt;p&gt;Fortunately, the new Jenkins Pipeline job feature addresses these drawbacks head-on.&lt;/p&gt; &lt;h2&gt;Integrating CI/CD and GitOps with Jenkinsfiles&lt;/h2&gt; &lt;p&gt;A &lt;em&gt;Jenkinsfile&lt;/em&gt; is a text file, written in the &lt;a href="https://groovy-lang.org/"&gt;Groovy&lt;/a&gt; programming language, that defines each step in a Jenkins job. Usually, a Jenkinsfile is created by a developer or system administrator with a detailed understanding of how to deploy the particular component or application.&lt;/p&gt; &lt;p&gt;Once a Jenkinsfile is created, it is committed to a repository in the version control system that’s hosting the source code. After the Jenkinsfile is committed, the developer or sysadmin creates a job in Jenkins that declares the location of the Jenkinsfile in the source code repository and instructs Jenkins when to execute the Jenkinsfile. That’s it. There is no extensive twiddling with configuration settings in the user interface (UI). The Jenkinsfile has all the instructions required to run the job. (See Figure 1.)&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/file.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/file.png?itok=4mn91ZA_" width="883" height="307" alt="The Jenkinsfile describing an application’s deployment process is imported from a source control management system and executed by the Jenkins server." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1. The Jenkinsfile is imported from a source control management system (SCM) and executed by the Jenkins server. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Using a Jenkinsfile makes transferring jobs between servers much easier. All that’s required is to spin up a new job in Jenkins, bind that job to the Jenkinsfile that’s stored in version control, and then declare when the job is supposed to run. All the details and intelligence about the deployment are defined in the Jenkinsfile.&lt;/p&gt; &lt;p&gt;For all intents and purposes, the Jenkinsfile is the single source of truth (SSOT) about how a deployment executes. And that SSOT is hosted in the version control repository. Putting the SSOT in a version control repository is emblematic of the GitOps way of doing things, so let's talk about that next.&lt;/p&gt; &lt;h2&gt;GitOps and the single source of truth&lt;/h2&gt; &lt;p&gt;In a GitOps-driven deployment process, all activities emanate from the version-controlled code repository. Some companies drive their GitOps deployment process directly within the particular code repository service, such as GitHub, BitBucket, or Gitlab. Other companies have an external agent such as Jenkins execute the deployment process.&lt;/p&gt; &lt;p&gt;Regardless of the approach you choose, the important thing to understand about GitOps is that the single source of truth for all deployment activity is the code repository. Using a Jenkinsfile that’s hosted in a repository to define a job that runs under Jenkins fits well with the GitOps sensibility.&lt;/p&gt; &lt;p&gt;Now that we’ve covered how Jenkins implements CI/CD, and how it fits into the GitOps way of doing things, let’s move to a concrete example. Over the next few sections, we will implement a CI/CD process by running a Jenkinsfile under a Jenkins Pipeline job.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: Are you curious about how &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt; and Jenkins Pipelines work together? We recommend &lt;a href="https://cloud.redhat.com/blog/jenkins-pipelines"&gt;Simply Explained: OpenShift and Jenkins Pipelines&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Deploy a Node.js application using a Jenkinsfile&lt;/h2&gt; &lt;p&gt;Recall that a Jenkinsfile is a text file that describes the details of a job that will run under Jenkins. This section presents a job that executes a three-stage deployment to build, test, and release a &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js&lt;/a&gt; application:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;The build stage gets the application source code from GitHub and installs the dependency packages.&lt;/li&gt; &lt;li&gt;The test stage tests the application.&lt;/li&gt; &lt;li&gt;The release stage encapsulates the application into a Docker image that is then stored in a local container repository.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The following Jenkinsfile does the work of creating the local container repository. Once the container image is stored in the container repository, the Jenkinsfile runs a Docker container from the stored container image:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;node { env.NODEJS_HOME = "${tool 'basic_node'}" // on linux / mac env.PATH="${env.NODEJS_HOME}/bin:${env.PATH}" sh 'npm --version' } pipeline { agent any stages { stage('build') { steps { git branch: 'main', url: 'https://github.com/reselbob/secret-agent.git' sh "npm install" } } stage('test') { steps { script { env.SECRET_AGENT_PORT = "3060" echo "SECRET_AGENT_PORT is '${SECRET_AGENT_PORT}'" } sh "npm test" } } stage('release') { steps { script { env.SECRET_AGENT_PORT = "3050" echo "SECRET_AGENT_PORT is '${SECRET_AGENT_PORT}'" } // If the local registry container does not exists, create it sh """ if ! [ \$(docker ps --format '{{.Names}}' | grep -w registry &amp;&gt; /dev/null) ]; then docker run -d --network='host' -p 5000:5000 --restart=always --name registry registry:2; fi; """ // if the secret_agent container is running, delete it in order to create a new one sh """ if [ \$(docker ps --format '{{.Names}}' | grep -w secret_agent &amp;&gt; /dev/null) ]; then docker rm -f secret_agent; fi; """ sh "docker build -t secretagent:v1 . " sh "docker tag secretagent:v1 localhost:5000/secretagent:v1 " sh "docker run -d --network='host' -p 3050:3050 --name secret_agent localhost:5000/secretagent:v1 " sh "echo 'Secret Agent up and running on port 3050' " } } } }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note that the Jenkinsfile has two root-level sections: &lt;code&gt;node&lt;/code&gt; and &lt;code&gt;pipeline&lt;/code&gt;. We'll look at these next.&lt;/p&gt; &lt;h3&gt;The node section of the Jenkinsfile&lt;/h3&gt; &lt;p&gt;The &lt;code&gt;node&lt;/code&gt; section is the first step in the deployment process. It establishes a workspace in the Jenkins server under which a deployment runs. The workspace runs a Node.js application.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; The word &lt;em&gt;node&lt;/em&gt; has two different meanings here. Used by itself in a Jenkinsfile, &lt;code&gt;node&lt;/code&gt; describes the workspace. When used in the variable &lt;code&gt;basic_node&lt;/code&gt;, it refers to the Node.js runtime environment.&lt;/p&gt; &lt;p&gt;In this example of the &lt;code&gt;node&lt;/code&gt; section, the Jenkinsfile adds the location of the Node.js package to the environment’s PATH and then executes &lt;code&gt;npm --version&lt;/code&gt; to verify that the Node.js package manager is installed and accessible globally. By implication, a successful &lt;code&gt;npm --version&lt;/code&gt; command demonstrates that Node.js is installed.&lt;/p&gt; &lt;p&gt;The &lt;code&gt;npm --version&lt;/code&gt; command is executed as a parameter to the &lt;code&gt;sh&lt;/code&gt; command. The &lt;code&gt;sh&lt;/code&gt; command is used in the Jenkinsfile to execute commands a developer typically runs in a terminal window at the command line.&lt;/p&gt; &lt;h3&gt;The pipeline section of the Jenkinsfile&lt;/h3&gt; &lt;p&gt;The &lt;code&gt;pipeline&lt;/code&gt; section that follows the &lt;code&gt;node&lt;/code&gt; section defines the deployment that executes in three stages: build, test, and release. Each stage has a &lt;code&gt;steps&lt;/code&gt; subsection that describes the tasks that need to be executed in that stage.&lt;/p&gt; &lt;h4&gt;Build, test, and release&lt;/h4&gt; &lt;p&gt;The &lt;code&gt;build&lt;/code&gt; stage clones the application from a source code repository and installs the Node.js dependency packages that the application requires.&lt;/p&gt; &lt;p&gt;The &lt;code&gt;test&lt;/code&gt; stage executes a &lt;code&gt;sh&lt;/code&gt; command that, in turn, runs a &lt;code&gt;npm test&lt;/code&gt; command that is special to the application. Executing &lt;code&gt;npm test&lt;/code&gt; runs the various tests that are defined within the Node.js application’s source code.&lt;/p&gt; &lt;p&gt;The instructions for the release stage are a bit more complicated. The first thing the release stage does is check whether the local container registry is running. If the local registry does not exist, the Jenkinsfile creates it.&lt;/p&gt; &lt;h4&gt;Conditional commands&lt;/h4&gt; &lt;p&gt;The next set of commands checks whether the container that will be created from the source code is already running. The container’s name is &lt;code&gt;secret_agent&lt;/code&gt;. If the container is running, the Jenkinsfile deletes it. This is done so that the Jenkins job can run repeatedly. If a second run of the Jenkins job were to encounter a running instance of the &lt;code&gt;secret_agent&lt;/code&gt; container, the job would fail. Thus, any existing &lt;code&gt;secret_agent&lt;/code&gt; container needs to be deleted.&lt;/p&gt; &lt;p&gt;Once all the conditional commands have been executed, the Jenkinsfile builds a Docker image for the &lt;code&gt;secret_agent&lt;/code&gt; code and pushes the image into the local registry. Then, an instance of the &lt;code&gt;secret_agent&lt;/code&gt; container is created using the image stored in the local container registry.&lt;/p&gt; &lt;p&gt;The important thing to note about the deployment process is that all the instructions relevant to running a job under Jenkins are defined in the Jenkinsfile. If you ever need to change any deployment instructions, you don’t need to fiddle around with the Jenkins UI. All you need to do is alter the Jenkinsfile. Isolating this work to the Jenkinsfile makes it easier to manage and audit changes to the deployment process.&lt;/p&gt; &lt;p&gt;Now we've covered the Jenkinsfile and its sections. The last thing you need to do is bind the Jenkinsfile to a Jenkins job. This is done from within the Jenkins UI, as described in the next section.&lt;/p&gt; &lt;h2&gt;Binding a Jenkinsfile to a Jenkins Pipeline job&lt;/h2&gt; &lt;p&gt;Binding a Jenkinsfile to a Jenkins Pipeline job is straightforward, as shown by the sequence of screens in Figure 2. Here are the steps:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Create a job in the Jenkins Dashboard.&lt;/li&gt; &lt;li&gt;Name the job.&lt;/li&gt; &lt;li&gt;Declare it a Jenkins Pipeline job.&lt;/li&gt; &lt;/ol&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/project.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/project.png?itok=OKu71eA5" width="1125" height="684" alt="On the Jenkins Dashboard, you create and name a job and mark it as a Jenkins Pipeline." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2. On the Jenkins Dashboard, you create and name a job and mark it as a Jenkins Pipeline. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;Once the Jenkins Pipeline job is created, it needs to be configured. Figure 3 shows the first two steps in the process:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Enter a description for the job.&lt;/li&gt; &lt;li&gt;Declare how often to check the source code repository for changes.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;In Figure 3, the job is configured to poll the source code repository every 15 minutes.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/config_0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/config_0.png?itok=8N-WC_CY" width="1068" height="569" alt="After entering a description, choose a build trigger." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3. After entering a description, choose a build trigger. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Once the description and polling interval are declared, complete the configuration that binds the Jenkinsfile to the job. The following steps are illustrated in Figure 4:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Select &lt;strong&gt;Pipeline script from SCM&lt;/strong&gt; from the first dropdown menu.&lt;/li&gt; &lt;li&gt;Select &lt;strong&gt;Git&lt;/strong&gt; from the next dropdown menu. Doing this reveals an additional set of text boxes where you can do the following: &lt;ul&gt;&lt;li&gt;Enter the URL of the version code repository that has the Jenkinsfile.&lt;/li&gt; &lt;li&gt;Enter the branch that has the version of the Jenkinsfile of interest.&lt;/li&gt; &lt;li&gt;Declare the name of the Jenkinsfile.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;Figure 4 uses the default file, simply named &lt;code&gt;Jenkinsfile&lt;/code&gt;.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/job.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/job.png?itok=YTb8ARTK" width="973" height="688" alt="Configure the Jenkins Pipeline job with the URL and branch of the repository and the the name of the Jenkinsfile." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4. Configure the Jenkins Pipeline job with the URL and branch of the repository and the name of the Jenkinsfile. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;You can have many alternative Jenkinsfiles, with names such as &lt;code&gt;Jenkinsfile_k8s&lt;/code&gt; or &lt;code&gt;Jenkinsfile_windows&lt;/code&gt;. This means that you can use the same source code repository for a variety of jobs. Each job will execute its own build instructions described by the relevant Jenkinsfile.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; The versatility that comes with using many different versions of Jenkinsfiles is particularly useful for working in application environments that have complex provisioning and configuration requirements. For example, &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; and OpenShift configurations support a wide variety of settings—everything from container configuration to security rules. Putting the provisioning and configuration settings in one Jenkinsfile can become a maintenance headache. But, splitting up configurations among many files in a way that is particular to each environment makes deployment management a lot easier. The savings in labor alone can be significant.&lt;/p&gt; &lt;p&gt;Once the Jenkins Pipeline job is configured, you can run it from the Jenkins Dashboard. As shown in Figure 5, select the Jenkins Pipeline job, then click the &lt;strong&gt;Build Now&lt;/strong&gt; menu button to run it. The job runs and the result of each stage is shown in the job’s page on the Jenkins Dashboard.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/result.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/result.png?itok=rtSL3o7z" width="1315" height="812" alt="The results of executing a Jenkinsfile are shown in the job's Jenkins Dashboard page." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 5. The results of executing a Jenkinsfile are shown in the job's Jenkins Dashboard page. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The beauty of it all is that the job's runtime details are described in the associated Jenkinsfile. Should the deployment process need to change, all you need to do is change the Jenkinsfile in the source code repository.&lt;/p&gt; &lt;h2&gt;The benefits of using Jenkins Pipelines for GitOps and CI/CD&lt;/h2&gt; &lt;p&gt;There are many benefits to using Jenkins Pipeline jobs in conjunction with a version control server such as GitHub, and thus combining CI/CD and GitOps.&lt;/p&gt; &lt;p&gt;First, putting deployment instructions in a central version control repository makes the repository a central location, which can be used as a single source of truth for all deployment activities. Using the repository as a single source of truth provides reliable change management and auditing capabilities. Version control and access security are also built into the service.&lt;/p&gt; &lt;p&gt;Second, declaring the build process in a Jenkinsfile makes it easier to automate deployment. You don’t have to fiddle with a UI to get your programs out; you can just write the code and let the Jenkins Pipeline do the rest.&lt;/p&gt; &lt;p&gt;Finally, Jenkins is a well-known CI/CD tool. It has all the features that are required for implementing a viable GitOps-focused deployment process. For companies already using Jenkins, making the leap to GitOps using a Jenkinsfile is much easier than starting from scratch. And for companies not yet using Jenkins, the learning curve is acceptable. This technology has proven itself over the years, and there are many &lt;a href="https://developers.redhat.com/courses/gitops"&gt;learning resources and examples&lt;/a&gt; developers can use to get up to speed.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/topics/gitops"&gt;GitOps&lt;/a&gt; has a lot to offer developers who want to automate their deployment processes using scripts that are stored as a single source of truth in a version control management system. Such control and reliability are compelling reasons to make the move.&lt;/p&gt; &lt;p&gt;You can't adopt GitOps in a day or two. It takes time to get the organizational processes in place. But many developers and their organizations will find that the time required to get GitOps working under a Jenkins Pipeline job, using a Jenkinsfile, is a good investment, especially for the benefits at hand.&lt;/p&gt; &lt;h2&gt;Learn more about GitOps and CI/CD on Red Hat Developer&lt;/h2&gt; &lt;p&gt;Visit the following articles and resources for more about using GitOps and CI/CD for secure, automated deployments:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Red Hat Developer learning courses: &lt;a href="https://developers.redhat.com/courses/gitops"&gt;Develop with GitOps&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/blog/2021/05/13/why-should-developers-care-about-gitops"&gt;Why should developers care about GitOps?&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/06/29/how-apply-machine-learning-gitops"&gt;How to apply machine learning to GitOps&lt;/a&gt;&lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/articles/2021/08/03/managing-gitops-control-planes-secure-gitops-practices"&gt;Managing GitOps control planes for secure GitOps practices&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/articles/2021/06/21/modern-fortune-teller-using-gitops-automate-application-deployment-red-hat"&gt;Modern Fortune Teller: Using GitOps to automate application deployment on Red Hat OpenShift&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/01/13/developers-guide-cicd-and-gitops-jenkins-pipelines" title="A developer's guide to CI/CD and GitOps with Jenkins Pipelines"&gt;A developer's guide to CI/CD and GitOps with Jenkins Pipelines&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Bob Reselman</dc:creator><dc:date>2022-01-13T07:00:00Z</dc:date></entry><entry><title type="html">This Week in JBoss - January 13th 2022</title><link rel="alternate" href="https://www.jboss.org/posts/weekly-2022-01-13.html" /><category term="quarkus" /><category term="java" /><category term="resteasy" /><category term="camel" /><category term="reactive" /><category term="panache" /><category term="hibernate" /><category term="event-driven" /><category term="netty" /><category term="kubernetes" /><author><name>Stefan Sitani</name><uri>https://www.jboss.org/people/stefan-sitani</uri><email>do-not-reply@jboss.com</email></author><id>https://www.jboss.org/posts/weekly-2022-01-13.html</id><updated>2022-01-13T00:00:00Z</updated><content type="html">&lt;article class="" data-tags="quarkus, java, resteasy, camel, reactive, panache, hibernate, event-driven, netty, kubernetes"&gt; &lt;h1&gt;This Week in JBoss - January 13th 2022&lt;/h1&gt; &lt;p class="preamble"&gt;&lt;/p&gt;&lt;p&gt;Happy new year, everyone! Welcome to the first JBoss weekly editorial of 2022. Enjoy our pick of the latest news and interesting reads from around the JBoss community.&lt;/p&gt;&lt;p&gt;&lt;/p&gt; &lt;div class="sect1"&gt; &lt;h2 id="_releases_releases_releases"&gt;Releases, releases, releases!&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;Here are the releases from the JBoss Community for this edition:&lt;/p&gt; &lt;div class="ulist square"&gt; &lt;ul class="square"&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://quarkus.io/blog/quarkus-2-6-0-final-released/"&gt;Quarkus 2.6.0&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://camel.apache.org/blog/2021/12/camel-quarkus-release-2.6.0/"&gt;Camel Quarkus 2.6.0&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://debezium.io/blog/2021/12/16/debezium-1.8-final-released/"&gt;Debezium 1.8.0&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://vertx.io/blog/eclipse-vert-x-3-9-12/"&gt;Eclipse Vert.x 3.9.12&lt;/a&gt; and &lt;a href="https://vertx.io/blog/eclipse-vert-x-4-2-3/"&gt;4.2.3&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://in.relation.to/2022/01/05/hibernate-search-6-0-8-Final/"&gt;Hibernate Search 6.0.8&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_articles_books_and_tutorials"&gt;Articles, Books, and Tutorials&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;New year, new content! Check out some of the recent articles about Java, Kubernetes, Quarkus, and more…​&lt;/p&gt; &lt;div class="sect2"&gt; &lt;h3 id="_new_book_reactive_systems_in_java"&gt;New Book: Reactive Systems in Java&lt;/h3&gt; &lt;p&gt;&lt;a href="https://www.oreilly.com/library/view/reactive-systems-in/9781492091714/"&gt;Reactive Systems in Java&lt;/a&gt;, by Clement Escoffier and Ken Finnigan&lt;/p&gt; &lt;p&gt;In their new book &lt;em&gt;Reactive Systems in Java&lt;/em&gt;, Clement Escoffier and Ken Finnigan take an in-depth look at reactive systems design make applications responsive, elastic, and resilient, and explore event-driven architectures as a flexible and composable option for designing distributed systems. Full of practical examples, their new book helps developers bring these approaches together when designing applications with Quarkus.&lt;/p&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_simplify_java_persistence_using_quarkus_and_hibernate_reactive"&gt;Simplify Java persistence using Quarkus and Hibernate Reactive&lt;/h3&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/articles/2022/01/06/simplify-java-persistence-using-quarkus-and-hibernate-reactive#"&gt;Simplify Java persistence using Quarkus and Hibernate Reactive&lt;/a&gt; by Daniel Oh&lt;/p&gt; &lt;p&gt;Daniel Oh walks you through developing a reactive application with persistent data storage that leverages the benefits of simplified JPA implementation using the new extension for Hibernate Reactive with Panache.&lt;/p&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_getting_started_with_netty"&gt;Getting started with Netty&lt;/h3&gt; &lt;p&gt;&lt;a href="http://www.mastertheboss.com/jboss-frameworks/netty/jboss-netty-tutorial/?utm_source=rss&amp;#38;utm_medium=rss&amp;#38;utm_campaign=jboss-netty-tutorial"&gt;Getting started with Netty&lt;/a&gt; by Francesco Marchioni&lt;/p&gt; &lt;p&gt;Francesco’s tutorial introduces you to Netty a client/server framework that provides a simplified layer over non-blocking I/O networking. The introduction briefly describes the notable features that Netty provides. The first part of the tutorial section explains how you can create a simple echo server. The second part shows you how you can expand it by adding a separate Netty Client with its own Handler, and also demonstrates how to send a String between the client and the server over the event loop.&lt;/p&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_why_you_should_migrate_your_java_workloads_to_openshift"&gt;Why you should migrate your Java workloads to OpenShift&lt;/h3&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/articles/2022/01/07/why-you-should-migrate-your-java-workloads-openshift#"&gt;Why you should migrate your Java workloads to OpenShift&lt;/a&gt; by Philip Hayes&lt;/p&gt; &lt;p&gt;In the first part of his two-part post series, Philip Hayes outlines the main benefits of migrating Java workloads to OpenShift and explores a number of tools available from Red Hat and from the Konveyor community project that are designed to help you transitions your Java workloads to the cloud with ease. Stay tuned for part 2 that will take an in-depth look at migrating a JBoss EAP application to OpenShift!&lt;/p&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_how_to_use_quarkus_with_the_service_binding_operator"&gt;How to use Quarkus with the Service Binding Operator&lt;/h3&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/articles/2021/12/22/how-use-quarkus-service-binding-operator"&gt;How to use Quarkus with the Service Binding Operator&lt;/a&gt; by Ioannis Kanellos&lt;/p&gt; &lt;p&gt;Ioannis Kanellos opens his article with a brief history of service binding solutions for Kubernetes developers and follows it up with a demonstration of a developer workflow centered around binding an external database service to an application in a cloud environment. With plenty of code examples and technical detail, Ioannis walks you through provisioning a database service, writing a Quarkus app for simplified data access with Hibernate with Panache, generating the service binding resources using the Service Binding Operator, and deploying your project to a containerized Kubernetes cluster running on your local machine.&lt;/p&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_videos"&gt;Videos&lt;/h3&gt; &lt;p&gt;Here’s my pick of this week’s YouTube videos:&lt;/p&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://youtu.be/1_GUWDeQIsc"&gt;Drag and Drop your Quarkus Serverless App on the Developer Sandbox&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://youtu.be/1Rt7CPFodbE"&gt;Quarkus Insights #75: State of the Quarkus Ecosystem&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;p&gt;&lt;em&gt;That’s all for today! Please join us again in two weeks for another round of our JBoss editorial!&lt;/em&gt;&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="author"&gt; &lt;pfe-avatar pfe-shape="circle" pfe-pattern="squares" pfe-src="/img/people/stefan-sitani.png"&gt;&lt;/pfe-avatar&gt; &lt;span&gt;Stefan Sitani&lt;/span&gt; &lt;/div&gt;&lt;/article&gt;</content><dc:creator>Stefan Sitani</dc:creator></entry><entry><title>Integrate Apache ActiveMQ brokers using Camel K</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/01/12/integrate-apache-activemq-brokers-using-camel-k" /><author><name>Maz Arslan, Anton Giertli</name></author><id>09666143-7ea3-489e-a486-4929d1682e5d</id><updated>2022-01-12T07:00:00Z</updated><published>2022-01-12T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://activemq.apache.org"&gt;Apache ActiveMQ&lt;/a&gt; is a highly popular message broker that features persistence, guaranteed message delivery, and high throughput. &lt;a href="https://activemq.apache.org/components/artemis/"&gt;Apache ActiveMQ Artemis&lt;/a&gt; streamlines the classic message broker implementation for &lt;a href="https://developers.redhat.com/topics/microservices"&gt;microservices&lt;/a&gt; architectures. This article is for developers transitioning from &lt;a href="https://activemq.apache.org/components/classic/"&gt;ActiveMQ Classic&lt;/a&gt; to &lt;a href="https://activemq.apache.org/components/artemis/"&gt;ActiveMQ Artemis&lt;/a&gt;. We will show you how to get the two versions working together using &lt;a href="https://camel.apache.org/camel-k/1.7.x/index.html"&gt;Apache Camel K&lt;/a&gt;. Our example is based on &lt;a href="https://developers.redhat.com/products/amq/overview"&gt;Red Hat AMQ&lt;/a&gt; versions 6 and 7, and we will perform the steps on &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift 4&lt;/a&gt;. Our code is written in &lt;a href="https://developers.redhat.com/enterprise-java"&gt;Java&lt;/a&gt;. The integration process and techniques should be applicable to many other scenarios.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; See &lt;a href="https://developers.redhat.com/articles/2021/06/30/implementing-apache-activemq-style-broker-meshes-apache-artemis"&gt;Implementing Apache ActiveMQ-style broker meshes with Apache Artemis&lt;/a&gt; for a discussion of the differences between Red Hat AMQ 6 and 7.&lt;/p&gt; &lt;h2&gt;The Camel K integration workflow&lt;/h2&gt; &lt;p&gt;Camel K is an integration framework that allows developers to exchange data easily between many common data processing tools. The demonstration in this article creates two ActiveMQ brokers, one using Red Hat AMQ 6 and the other using Red Hat AMQ 7. We then create two Camel K integrations: One to help AMQ 7 consume messages, and another to help AMQ 6 produce them. Finally, we create a third integration that functions as a message bridge between the two brokers.&lt;/p&gt; &lt;p&gt;Figure 1 illustrates the Camel K integration workflow.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/AMQSSL_CAMELK%20Diagram.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/AMQSSL_CAMELK%20Diagram.png?itok=ygU8KNpA" width="960" height="540" alt="Figure 1. A message bridge connects two ActiveMQ brokers." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1. A message bridge connects two ActiveMQ brokers.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;You can retrieve the files for the demonstration from the &lt;a href="https://github.com/MazArslan/CamelK-SSL-Demo"&gt;GitHub repository&lt;/a&gt; associated with this article.&lt;/p&gt; &lt;h2&gt;Prerequisites&lt;/h2&gt; &lt;p&gt;This article assumes you have a basic knowledge of the OpenShift &lt;code&gt;oc&lt;/code&gt; command-line interface and OpenShift web console. You need a running instance of OpenShift 4 and admin access on the OpenShift cluster. You will also need to &lt;a href="https://camel.apache.org/download/"&gt;install Camel K&lt;/a&gt; on your local system.&lt;/p&gt; &lt;h2&gt;Set up the cluster&lt;/h2&gt; &lt;p&gt;Create a namespace on your OpenShift instance:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc new-project camelk-ssl&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;As the cluster administrator, install the &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_integration/2020-q4/html/deploying_camel_k_integrations_on_openshift/installing-camel-k"&gt;Red Hat Camel K Operator&lt;/a&gt; and &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_amq/7.7/html-single/deploying_amq_broker_on_openshift/index"&gt;Red Hat AMQ 7.7 Operator&lt;/a&gt; on your OpenShift instance from the OpenShift OperatorHub.&lt;/p&gt; &lt;p&gt;Now, create certificates to enable secure SSL communication. Make sure you set the common name &lt;code&gt;CN&lt;/code&gt; on the broker to your server domain name:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt; export CLIENT_KEYSTORE_PASSWORD=password export CLIENT_TRUSTSTORE_PASSWORD=password export BROKER_KEYSTORE_PASSWORD=password export BROKER_TRUSTSTORE_PASSWORD=password #Client Keystore keytool -genkey -alias client -keyalg RSA -keystore client.ks -storepass $CLIENT_KEYSTORE_PASSWORD -keypass $CLIENT_KEYSTORE_PASSWORD -dname "CN=camelssl-example, O=RedHat, C=UK" #Broker Keystore keytool -genkey -alias broker -keyalg RSA -keystore broker.ks -storepass $BROKER_KEYSTORE_PASSWORD -keypass $BROKER_KEYSTORE_PASSWORD -dname "CN=*.apps.cluster-hhl6r.hhl6r.sandbox55.opentlc.com, O=RedHat, C=UK" #Export Client PublicKey keytool -export -alias client -keystore client.ks -storepass $CLIENT_KEYSTORE_PASSWORD -file client.cert #Export Server PublicKey keytool -export -alias broker -keystore broker.ks -storepass $BROKER_KEYSTORE_PASSWORD -file broker.cert #Import Server PublicKey into Client Truststore keytool -import -alias broker -keystore client.ts -file broker.cert -storepass $CLIENT_TRUSTSTORE_PASSWORD -trustcacerts -noprompt #Import Client PublicKey into Server Truststore keytool -import -alias client -keystore broker.ts -file client.cert -storepass $BROKER_TRUSTSTORE_PASSWORD -trustcacerts -noprompt #Import Server PublicKey into Server Truststore (i.e.: trusts its self) keytool -import -alias broker -keystore broker.ts -file broker.cert -storepass $BROKER_TRUSTSTORE_PASSWORD -trustcacerts -noprompt &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Set up the ActiveMQ consumer (AMQ 7)&lt;/h2&gt; &lt;p&gt;Create a secret for the broker keystore and truststore for AMQ 7:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc create secret generic example-amq-secret --from-file=broker.ks --from-literal=keyStorePassword=password --from-file=client.ts=broker.ts --from-literal=trustStorePassword=password&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Create an AMQ 7 broker using the AMQ broker operator:&lt;/p&gt; &lt;pre&gt; &lt;code&gt; apiVersion: broker.amq.io/v2alpha4 kind: ActiveMQArtemis metadata: name: example-amq application: example-amq namespace: camelk-ssl spec: deploymentPlan: size: 1 image: registry.redhat.io/amq7/amq-broker:7.6 requireLogin: false adminUser: admin adminPassword: admin console: expose: true acceptors: - name: amqp protocols: amqp port: 5672 sslEnabled: true sslSecret: example-amq-secret verifyHost: false expose: true &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Create a new AMQ broker address. This configuration also creates a route and service for the broker:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;apiVersion: broker.amq.io/v2alpha2 kind: ActiveMQArtemisAddress metadata: name: example-testaddress spec: addressName: test queueName: test routingType: anycast &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To test your AMQ 7 broker, you have to download &lt;a href="https://activemq.apache.org/components/artemis/download/"&gt;ActiveMQ Artemis&lt;/a&gt;. Once the download is completed, extract it and open the &lt;code&gt;/bin&lt;/code&gt; folder, where you will find the Apache Artemis script that is used for testing. You can produce and consume messages using the new AMQ 7 broker from your local machine; just make sure to change the URL and truststore location.&lt;/p&gt; &lt;p&gt;Here is a sample command to produce messages:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ./artemis producer --url 'amqps://example-amq7-amqp-0-svc-rte-camelk-ssl.apps.cluster-hhl6r.hhl6r.sandbox55.opentlc.com:443?jms.username=admin&amp;jms.password=admin&amp;transport.trustStoreLocation=/home/marslan/work/camelkssl/client.ts&amp;transport.trustStorePassword=password&amp;transport.verifyHost=false' --threads 1 --protocol amqp --message-count 10 --destination 'queue://test'&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Here is a sample command to consume messages:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$./artemis consumer --url 'amqps://example-amq7-amqp-0-svc-rte-camelk-ssl.apps.cluster-hhl6r.hhl6r.sandbox55.opentlc.com:443?jms.username=admin&amp;jms.password=admin&amp;transport.trustStoreLocation=/home/marslan/work/camelkssl/client.ts&amp;transport.trustStorePassword=password&amp;transport.verifyHost=false' --threads 1 --protocol amqp --message-count 10 --destination 'queue://test'&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Set up the ActiveMQ producer (AMQ 6)&lt;/h2&gt; &lt;p&gt;Next, create a secret for the broker keystore and truststore for AMQ 6:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc create secret generic example-amq6-secret --from-file=broker.ks --from-literal=keyStorePassword=password --from-file=broker.ts --from-literal=trustStorePassword=password &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Create an AMQ 6 broker from the command line:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc new-app amq63-ssl -p APPLICATION_NAME=amq6-broker -p MQ_QUEUES=test -p MQ_TOPICS=test -p MQ_USERNAME=admin \ -p MQ_PASSWORD=admin -p ActiveMQ_SECRET=example-amq6-secret -p AMQ_TRUSTSTORE=broker.ts -p AMQ_TRUSTSTORE_PASSWORD=password \ -p AMQ_KEYSTORE=broker.ks -p AMQ_KEYSTORE_PASSWORD=password -n camelk-ssl&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Create a route for the broker in the OpenShift console, using the &lt;code&gt;amq6-broker-tcp-ssl&lt;/code&gt; service.&lt;/p&gt; &lt;p&gt;To test your AMQ 6 broker, you can download &lt;a href="https://activemq.apache.org/components/classic/download/"&gt;ActiveMQ&lt;/a&gt;. Once the download is completed, extract it and open the &lt;code&gt;/bin&lt;/code&gt; folder, where you will find the ActiveMQ script that is used for testing. You can produce and consume messages using the new AMQ 6 broker from your local machine. Just make sure to change the URL and truststore location.&lt;/p&gt; &lt;p&gt;Enter the following command to produce messages:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ./activemq producer \ -Djavax.net.ssl.trustStore=/home/marslan/work/camelkssl/client.ts \ -Djavax.net.ssl.trustStorePassword=password \ -Djavax.net.ssl.keyStore=/home/marslan/work/camelkssl/client.ks \ -Djavax.net.ssl.keyStorePassword=password \ --brokerUrl ssl://test-camelk-ssl.apps.cluster-hhl6r.hhl6r.sandbox55.opentlc.com:443 \ --user admin \ --password admin \ --destination queue://test \ --messageCount 1000 \ --message HelloWorld &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Enter the following command to consume messages:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ./activemq consumer \ -Djavax.net.ssl.trustStore=/home/marslan/work/camelkssl/client.ts \ -Djavax.net.ssl.trustStorePassword=password \ -Djavax.net.ssl.keyStore=/home/marslan/work/camelkssl/client.ks \ -Djavax.net.ssl.keyStorePassword=password \ --brokerUrl ssl://test-camelk-ssl.apps.cluster-hhl6r.hhl6r.sandbox55.opentlc.com:443 \ --user admin \ --password admin \ --destination queue://test &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Test Camel K&lt;/h2&gt; &lt;p&gt;Now, we will import a simple Java application and use it to ensure that Camel K is working correctly. The &lt;code&gt;HelloCamelK.java&lt;/code&gt; program is available in the &lt;a href="https://github.com/MazArslan/CamelK-SSL-Demo"&gt;Camel K SSL Demo repository&lt;/a&gt; on GitHub.&lt;/p&gt; &lt;p&gt;Run the file with the following command (you may delete this integration after the test):&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ kamel run HelloCamelK&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Create the AMQ 7 consumer&lt;/h2&gt; &lt;p&gt;Create folders for AMQ 6 and AMQ 7:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ mkdir amq6 $ mkdir amq7&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Create a new configuration directory for AMQ 7:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ mkdir amq7/configs&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In the configuration directory, create an &lt;code&gt;application.properties&lt;/code&gt; file for the AMQ 7 integration, with the following contents:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;quarkus.qpid-jms.url=amqps://example-amq7-amqp-0-svc-rte-camelk-ssl.apps.cluster-hhl6r.hhl6r.sandbox55.opentlc.com:443?transport.keyStoreLocation=/etc/ssl/example-amq-secret/broker.ks&amp;transport.keyStorePassword=password&amp;transport.trustStoreLocation=/etc/ssl/example-amq-secret/client.ts&amp;transport.trustStorePassword=password&amp;transport.verifyHost=false quarkus.qpid-jms.username=admin quarkus.qpid-jms.password=admin jms.destinationType=queue jms.destinationName=test &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Within the &lt;code&gt;amq7&lt;/code&gt; folder, create a program named &lt;code&gt;amq7consumer.java&lt;/code&gt; for the AMQ 7 consumer integration:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;// camel-k: language=java &lt;1&gt; // camel-k: dependency=mvn:org.amqphub.quarkus:quarkus-qpid-jms &lt;2&gt; import org.apache.camel.builder.RouteBuilder; public class amq7consumer extends RouteBuilder { @Override public void configure() throws Exception { from("jms:{{jms.destinationType}}:{{jms.destinationName}}").to("log:info"); &lt;3&gt; } } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Let's look at a few parts of the code:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;camel-k: language=java&lt;/code&gt; provides information to Camel K to run the Java file. The comment enables Camel K to sort out dependencies.&lt;/li&gt; &lt;li&gt;&lt;code&gt;camel-k: dependency=mvn:org.amqphub.quarkus:quarkus-qpid-jms&lt;/code&gt; supports out-of-the-box configuration for SSL, without requiring any further customization. You can find more information in the &lt;a href="https://qpid.apache.org/releases/qpid-jms-0.54.0/docs/index.html#ssl-transport-configuration-options"&gt;Apache SSL documentation&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;The code consumes the &lt;code&gt;jms.url&lt;/code&gt;, &lt;code&gt;jms.destinationType&lt;/code&gt;, and &lt;code&gt;jms.destinationName&lt;/code&gt; properties specified in the &lt;code&gt;application.properties&lt;/code&gt; file and prints the values to the log.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The Java file is imported in the next step, where we run Camel K:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ kamel run -n camelk-ssl --property file:./amq7/configs/application.properties --resource secret:example-amq-secret@/etc/ssl/example-amq-secret amq7/amq7consumer.java &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In the &lt;code&gt;kamel run&lt;/code&gt; command:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;The &lt;code&gt;--property file&lt;/code&gt; option imports properties to Camel.&lt;/li&gt; &lt;li&gt;The &lt;code&gt;--resource&lt;/code&gt; option adds resources to the cluster pod. In this command, we are adding &lt;code&gt;example-amq-secret&lt;/code&gt; and placing its contents into the &lt;code&gt;/etc/ssl/example-amq-secret&lt;/code&gt; folder within the pod.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;To debug Camel K, add the following option to the &lt;code&gt;kamel run&lt;/code&gt; command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;--trait jvm.options=-Djavax.net.debug=all&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Create the AMQ 6 producer&lt;/h2&gt; &lt;p&gt;Create a new configuration directory for AMQ 6:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ mkdir amq6/configs&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In the configuration directory, create an &lt;code&gt;application.properties&lt;/code&gt; file for the AMQ 6 integration, with the following contents:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;activemq.destination.brokerURL=ssl://test-camelk-ssl.apps.cluster-hhl6r.hhl6r.sandbox55.opentlc.com:443 activemq.destination.username=admin activemq.destination.password=admin activemq.destination.ssl.keyStorePassword=password activemq.destination.ssl.keyStoreLocation=/etc/ssl/example-amq6-secret/broker.ks activemq.destination.ssl.trustStorePassword=password activemq.destination.ssl.trustStoreLocation=/etc/ssl/example-amq6-secret/broker.ts activemq.destination.type=queue activemq.destination.name=test &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Within the &lt;code&gt;amq6&lt;/code&gt; folder, create a program named &lt;code&gt;amq6SSLproducer.java&lt;/code&gt; for the AMQ 6 producer integration:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;// camel-k: dependency=camel:camel-quarkus-activemq // camel-k: dependency=camel:camel-quarkus-timer // camel-k: property=period=5000 public class amq6SSLProducer extends RouteBuilder { @Override public void configure() throws Exception { from("timer:foo?fixedRate=true&amp;period={{period}}").bean(this, "generateFakePerson()").to("log:info") .to("activemq:{{activemq.destination.type}}:{{activemq.destination.name}}?connectionFactory=#pooledConnectionFactory"); } public String generateFakePerson() { Faker faker = new Faker(); return faker.name().fullName() + " lives on " + faker.address().streetAddress(); } @ApplicationScoped public ActiveMQComponent activeMq(PooledConnectionFactory pooledConnectionFactory) { ActiveMQComponent activeMqComponent = new ActiveMQComponent(); activeMqComponent.setConnectionFactory(pooledConnectionFactory); activeMqComponent.setCacheLevelName("CACHE_CONSUMER"); return activeMqComponent; } @BindToRegistry public PooledConnectionFactory pooledConnectionFactory() throws Exception { return new PooledConnectionFactory(sslConnectionFactory()); } private ActiveMQSslConnectionFactory sslConnectionFactory() throws Exception { ActiveMQSslConnectionFactory connectionFactory = new ActiveMQSslConnectionFactory(); logger.info("BrokerURL: " + destinationBrokerURL); connectionFactory.setBrokerURL(destinationBrokerURL); connectionFactory.setUserName(destinationUserName); connectionFactory.setPassword(destinationPassword); connectionFactory.setTrustStore(destinationTrustStoreLocation); connectionFactory.setTrustStorePassword(destinationTruststorePassword); connectionFactory.setKeyStore(destinationKeyStoreLocation); connectionFactory.setKeyStorePassword(destinationKeystorePassword); return connectionFactory; } }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Run the producer using Camel K:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ kamel run -n camelk-ssl \ --property file:./amq6/configs/application.properties \ --resource secret:example-amq6-secret@/etc/ssl/example-amq6-secret amq6/amqssl.java&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Create a message bridge between AMQ 6 and AMQ 7&lt;/h2&gt; &lt;p&gt;Create new folders for the message bridge and its configuration:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ mkdir message-bridge $ mkdir message-bridge/configs&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In the configuration directory, create an &lt;code&gt;application.properties&lt;/code&gt; file for the message bridge, with the following contents:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;#amq6 activemq.source.brokerURL=ssl://test-camelk-ssl.apps.cluster-hhl6r.hhl6r.sandbox55.opentlc.com:443 activemq.source.username=admin activemq.source.password=admin activemq.source.ssl.keyStorePassword=password activemq.source.ssl.keyStoreLocation=/etc/ssl/example-amq6-secret/broker.ks activemq.source.ssl.trustStorePassword=password activemq.source.ssl.trustStoreLocation=/etc/ssl/example-amq6-secret/broker.ts activemq.source.type=queue activemq.source.name=test #amq7 quarkus.qpid-jms.url=amqps://example-amq7-amqp-0-svc-rte-camelk-ssl.apps.cluster-hhl6r.hhl6r.sandbox55.opentlc.com:443?transport.keyStoreLocation=/etc/ssl/example-amq-secret/broker.ks&amp;transport.keyStorePassword=password&amp;transport.trustStoreLocation=/etc/ssl/example-amq-secret/client.ts&amp;transport.trustStorePassword=password&amp;transport.verifyHost=false quarkus.qpid-jms.username=admin quarkus.qpid-jms.password=admin jms.destinationType=queue jms.destinationName=test&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In the &lt;code&gt;message-bridge&lt;/code&gt; folder, create a program named &lt;code&gt;sixToSevenBridge.java&lt;/code&gt;, with the following contents:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt; @Override public void configure() throws Exception { from("activeMQSource:{{activemq.source.type}}:{{activemq.source.name}}").to("log:info") .to("jms:{{jms.destinationType}}:{{jms.destinationName}}?connectionFactory=artemisConnectionFactory"); } @BindToRegistry("artemisConnectionFactory") public JmsConnectionFactory connectionFactory() throws Exception { return new JmsConnectionFactory(destinationBrokerURL); } @BindToRegistry("activeMQSource") public ActiveMQComponent activeMQSource() throws Exception{ ActiveMQComponent activeMqComponent = new ActiveMQComponent(); activeMqComponent.setConnectionFactory(pooledConnectionFactorySource()); activeMqComponent.setCacheLevelName("CACHE_CONSUMER"); return activeMqComponent; } @BindToRegistry("pooledConnectionFactorySource") public PooledConnectionFactory pooledConnectionFactorySource() throws Exception { return new PooledConnectionFactory(sslConnectionFactorySource()); } private ActiveMQSslConnectionFactory sslConnectionFactorySource() throws Exception { ActiveMQSslConnectionFactory connectionFactory = new ActiveMQSslConnectionFactory(); System.out.println("BrokerURL: " + sourceBrokerURL); connectionFactory.setBrokerURL(sourceBrokerURL); connectionFactory.setUserName(sourceUserName); connectionFactory.setPassword(sourcePassword); connectionFactory.setTrustStore(sourceTrustStoreLocation); connectionFactory.setTrustStorePassword(sourceTruststorePassword); connectionFactory.setKeyStore(sourceKeyStoreLocation); connectionFactory.setKeyStorePassword(sourceKeystorePassword); return connectionFactory; } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Run the message bridge:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ kamel run -n camelk-ssl \ --property file:./message-bridge/configs/application.properties message-bridge/sixToSevenBridge.java \ --resource secret:example-amq6-secret@/etc/ssl/example-amq6-secret \ --resource secret:example-amq-secret@/etc/ssl/example-amq-secret &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;View the logs to see whether the message bridge worked:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc logs -f {AMQ7 consumer pod} &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Clean up&lt;/h2&gt; &lt;p&gt;Uninstall the operators using the OpenShift command line:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc delete project camelk-ssl&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This article showed you how to use Camel K to connect different versions of the ActiveMQ message broker using Red Hat AMQ on OpenShift 4. Camel K is a highly effective integration framework that runs natively on &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt;. This article showcased just a few of its capabilities. Camel K allows you to build and &lt;a href="https://developers.redhat.com/topics/containers"&gt;containerize&lt;/a&gt; applications easily and quickly. Its integrations use very lightweight pods, thus consuming fewer resources.&lt;/p&gt; &lt;p&gt;Get more information from the &lt;a href="https://camel.apache.org/camel-k/next/"&gt;Camel K documentation&lt;/a&gt;. You might also enjoy the article &lt;a href="https://developers.redhat.com/blog/2020/05/12/six-reasons-to-love-camel-k"&gt;Six reasons to love Camel K&lt;/a&gt; and our series of related &lt;a href="https://developers.redhat.com/courses/camel-k"&gt;Camel K learning courses&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/01/12/integrate-apache-activemq-brokers-using-camel-k" title="Integrate Apache ActiveMQ brokers using Camel K"&gt;Integrate Apache ActiveMQ brokers using Camel K&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Maz Arslan, Anton Giertli</dc:creator><dc:date>2022-01-12T07:00:00Z</dc:date></entry><entry><title type="html">Kogito 1.15.0 released!</title><link rel="alternate" href="https://blog.kie.org/2022/01/kogito-1-15-0-released.html" /><author><name>Cristiano Nicolai</name></author><id>https://blog.kie.org/2022/01/kogito-1-15-0-released.html</id><updated>2022-01-12T06:36:03Z</updated><content type="html">We are glad to announce that the Kogito 1.15.0 release is now available! This goes hand in hand with, . From a feature point of view, we included a series of new features and bug fixes, including: * Quarkus Dev Service for Data Index, see * Enhanced support for multi instance sub-process. * Support using Quarkus config mecanish, including profiles and YAML files. * Serverless Workflow support for Workflow Compensation. * Async execution support for process nodes BREAKING CHANGES * Data Index PostgreSQL schema changes, please review release notes for more information. For more details head to the complete. All artifacts are available now: * Kogito runtime artifacts are available on Maven Central. * Kogito examples can be found. * Kogito images are available on. * Kogito operator is available in the in OpenShift and Kubernetes. * Kogito tooling 0.15.0 artifacts are available at the. A detailed changelog for 1.15.0 can be found in. New to Kogito? Check out our website. Click the "Get Started" button. The post appeared first on .</content><dc:creator>Cristiano Nicolai</dc:creator></entry><entry><title type="html">Intelligent data as a service (iDaaS) - Common architectural elements</title><link rel="alternate" href="http://www.schabell.org/2021/12/idaas-common-architectural-elements.html" /><author><name>Eric D. Schabell</name></author><id>http://www.schabell.org/2021/12/idaas-common-architectural-elements.html</id><updated>2022-01-12T06:00:00Z</updated><content type="html">Part 2 - Common architectural elements In  from this series we introduced an architecture around intelligent data as a service (iDaaS) for the healthcare industry. The process was laid out how we approached the use case and how portfolio solutions are the base for researching a generic architecture.  The only thing left to cover was the order in which you'll be led through the details. This article starts the real journey at the very top, with a generic architecture from which we'll discuss the common architectural elements one by one. ARCHITECTURE REVIEW As mentioned before, the architectural details covered here are based on real solutions using open source technologies. The example scenario presented here is a generic common architecture that was uncovered while researching those solutions. It's our intent to provide guidance and not deep technical details. This section covers the visual representations as presented, but it's expected that they'll be evolving based on future research. There are many ways to represent each element in this architecture, but we've chosen a format that we hope makes it easy to absorb. Feel free to post comments at the bottom of this post, or  with your feedback. Now let's take a look at the details in this architecture and outline the solution. FROM SPECIFIC TO GENERIC Before diving into the common elements, it might be nice to understand that this is not a catch all for every possible solution. It's a collection of identified elements that we've uncovered in multiple customer implementations. These elements presented here are then the generic common architectural elements that we've identified and collected into the generic architecture.  It's our intent to provide an architecture for guidance and not deep technical details. You're smart enough to figure out wiring integration points in your own architectures. You're capable of slotting in the technologies and components you've committed to in the past where applicable.  It's our job here to describe the architectural generic components and outline a few specific cases with visual diagrams so that you're able to make the right decisions from the start of your own projects. Another challenge has been how to visually represent the architecture. There are many ways to represent each element, but we've chosen some icons, text and colours that we hope are going to make it all easy to absorb. Now let's take a quick tour of the generic architecture and outline the common elements uncovered in our research. EDGE SERVICES Starting on the left side of the diagram, which is by no means a geographical necessity, there are two elements that represent external edge services that are integrated with the core elements of this architecture.  The first are edge devices, covering basically everything that is used in the field from clinical personnel, patients, to partnering vendors. These can be anything from sensor devices to mobile units such as phones or tablets, but certainly not limited to just these. It's a grouping to identify functionality on the edge of this use case. The second is a catch all for data collection and consumption called external data sources, a broad element containing all of the remote devices. For example, a heart monitoring machine capturing diagnostic information from a heart patient at home. IDAAS CORE These elements are core to the iDaaS solution and are deployed on a container platform to indicate the cloud-ready nature of this architecture.   Starting on the top right and working down from left to right, the first element is iDaaS knowledge insight. This represents the management applications that provide analytics and insights into the data available across the live platform. This can be setup to provide near-realtime gathering and reporting as organisational need require. This architecture relies on event streaming to react and respond to events within the iDaaS solution, making the iDaaS connect events element central to the solution. This is the element designed for collecting data streams and messages, making it all available to applications and services as subscribers, and pushing notifications as needed to interested parties when certain events happen. Core to any data architecture has to be integration. The iDaaS connect element represents the collection of both integration services and data integration services that are essential to bringing data, messages, and systems throughout a healthcare organisation together in a seamless fashion. The iDaaS connect data distribution element is a solution where routing of data to the desired destination happens. This can be incoming data being routed to the correct data store, or out bound data being routed to the right service, application, or end user. One of the most essential requirements for any healthcare organisation is to maintain compliancy to national laws, data privacy, and other transitional requirements. The iDaaS knowledge conformance element is a set of applications and tools that allow for any organisation to automate compliancy and regulation adherence using rule systems customised to their own local needs. This is a very flexible element that can be designed to ensure that compliancy audit requirements are constantly met. While data distribution can be a simple way to ensure data ends up in the right location, often there is a need for a more intelligent near real-time adjustment to the route data needs to take. Using the iDaaS intelligent data router allows for rule based intelligent routing based on message content, data origin, data destinations, or any other metric required to determine routing. For any healthcare organisation one can expect that there are some complexer processes that might require partially automated processing or even human interaction such as the . An element shown here as the iDaaS event builder is meant for capturing all the process automation needs for any healthcare organisation. It's easy to integrate with data, events, services, applications, and provides a treasure trove of processing metrics to help tune your organisations activities for patients and clinical staff. Finally, there is a need to provide access to internal services through generic application programming interfaces, or API's. The API management element provides all the needed functionality to help expose and connect with services, applications, and more. EXTERNAL SERVICES External services host an array of potential tools, systems, or third-party applications that are used in healthcare organisations. Every organisation has reporting services, either internal or external. These can be Software as a Service (SaaS) tools or just hosted tooling that is external to the organisation. There are any number of big data solutions and tools that can be found in healthcare architectures, often external to the organisation as they are working on very, very large data sets.  Monitoring &amp;amp; logging tools are in abundance together with analytics these also can be hosted externally applications, tooling, or interfaces to be integrated into a healthcare architecture. Many organisations are engaged with other partner data services and need to be able to interact in a timely fashion when sharing or pulling from these data sources. All of the above external services have a need to provide consistent access, which is done using an external API management element shown here that can be a SaaS offering or just hosted externally to the organisation. Finally, both security and any DevOps infrastructure or tooling can be hosted externally and needs to be integrated into the delivery processes for healthcare organisations that make use of them. WHAT'S NEXT This was just a short overview of the common generic elements that make up our architecture for the intelligent data as a service architecture.  An overview of this series on intelligent data as a service architecture: 1. 2. 3. Example data architecture 4. Example HL7 and FHIR integration architecture 5. Example iDaaS knowledge and insight architecture Catch up on any articles you missed by following one of the links above. Next in this series, taking a look at an example iDaaS architecture for the intelligent data as a service solution.</content><dc:creator>Eric D. Schabell</dc:creator></entry><entry><title>5 design principles for microservices</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/01/11/5-design-principles-microservices" /><author><name>Bob Reselman</name></author><id>fa3871c0-2c6a-48cd-a26e-729743bc6c7b</id><updated>2022-01-11T07:00:00Z</updated><published>2022-01-11T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/topics/microservices"&gt;Microservices&lt;/a&gt; are becoming increasingly popular to address shortcomings in monolithic applications. This article is the first in a three-part series that describes the design principles for a microservices-oriented application (MOA), shows how companies tend to evolve to use microservices, and describes the trade-offs in using microservices.&lt;/p&gt; &lt;h2&gt;What is a monolithic application?&lt;/h2&gt; &lt;p&gt;The term &lt;em&gt;monolithic&lt;/em&gt; applies to tightly integrated applications where it is hard to change one function without also recoding other parts of the application. Components in a monolithic application might be distributed among many machines, but they remain highly dependent on one another. Not only does the addition of a new feature have ripple effects throughout the code, but deploying the change requires retesting and redeploying the entire application. These upgrades can be labor-intensive and hazardous, particularly when an application has hundreds of thousands or even millions of users.&lt;/p&gt; &lt;p&gt;When IT departments had the luxury of releasing software every six months, this type of upgrade process was tolerable. But modern business demands force releases to happen weekly, daily, or even more often, so the labor and risk inherent in upgrading monolithic applications become untenable.&lt;/p&gt; &lt;p&gt;Something has to change. That change is the transformation to the microservices-oriented application (MOA).&lt;/p&gt; &lt;h2&gt;What is a microservices-oriented application?&lt;/h2&gt; &lt;p&gt;An MOA breaks its logic into small, well-encapsulated services that are distributed over several computing devices in a loosely coupled manner. Each service lives at a distinct IP address on the network and exposes a public interface that is language-agnostic. The most popular type of language-agnostic interface is a &lt;a href="https://www.redhat.com/en/topics/api/what-is-a-rest-api"&gt;REST API&lt;/a&gt;, but other models for communication exist. Microservices also generally get deployed as &lt;a href="https://developers.redhat.com/topics/containers"&gt;containers&lt;/a&gt; when it's time to go live.&lt;/p&gt; &lt;p&gt;Typically, some mechanism behind the scenes coordinates the microservices to create a unified application experience. Because each microservice is well-encapsulated, its code can be updated quickly with minimal side effects. This makes maintenance easier and scaling faster.&lt;/p&gt; &lt;p&gt;The benefits of an MOA can be significant, but they come with a price. You need to know a thing or two about microservice design to implement an MOA effectively—you can't make it up as you go along.&lt;/p&gt; &lt;p&gt;The purpose of this series is to describe the principles involved in choosing the microservices architecture, along with the pros and cons. This first article presents the five basic principles of microservices-oriented application design. The next part of the series explains the evolution of modern applications and why they lead to an MOA, and the third part finishes the series with trade-offs that microservices make.&lt;/p&gt; &lt;h2&gt;Five design principles for microservices&lt;/h2&gt; &lt;p&gt;The five basic principles of microservice application design are:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;A microservice has a single concern.&lt;/li&gt; &lt;li&gt;A microservice is discrete.&lt;/li&gt; &lt;li&gt;A microservice is transportable.&lt;/li&gt; &lt;li&gt;A microservice carries its own data.&lt;/li&gt; &lt;li&gt;A microservice is ephemeral.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Let’s look at the details.&lt;/p&gt; &lt;h3&gt;1. A microservice has a single concern&lt;/h3&gt; &lt;p&gt;Having a single concern means that a microservice should do one thing and one thing only. For example, if the microservice is intended to support authentication, it should do authentication only. This means that its interface should expose only access points that are relevant to authentication. And internally, the microservice should have authentication behavior only. For example, there should be no side behavior such as providing employee contact information in the authentication response.&lt;/p&gt; &lt;p&gt;Having a single concern makes the microservice easier to maintain and scale. Having a single concern also goes hand-in-hand with the next principle.&lt;/p&gt; &lt;h3&gt;2. A microservice is discrete&lt;/h3&gt; &lt;p&gt;A microservice must have clear boundaries separating it from its environment. Another way to think about this principle is that a microservice must be well-encapsulated. This means that all logic and data relevant to a microservice's single concern must be encapsulated into a single deployment unit. Examples of units for discrete deployment are a &lt;a href="https://developers.redhat.com/topics/linux/"&gt;Linux&lt;/a&gt; container, a WebAssembly binary, a &lt;a href="https://developers.redhat.com/topics/dotnet/"&gt;.NET&lt;/a&gt; DLL, a &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js&lt;/a&gt; package, and a &lt;a href="https://developers.redhat.com/topics/enterprise-java/"&gt;Java&lt;/a&gt; JAR file, to name a few.&lt;/p&gt; &lt;p&gt;Also, a discrete microservice is hosted in a distinct source control repository and is subject to its own &lt;a href="https://developers.redhat.com/topics/ci-cd/"&gt;CI/CD&lt;/a&gt; (continuous integration/continuous delivery) process. The microservice becomes part of a larger application after deployment. But from development through testing and on to release, each microservice is isolated from all other microservices. When a microservice is discrete, it becomes easily transportable, which is the next principle we'll cover.&lt;/p&gt; &lt;h3&gt;3. A microservice is transportable&lt;/h3&gt; &lt;p&gt;A transportable microservice can be moved from one runtime environment to another with little effort. Perhaps currently, the optimal form of a transportable microservice is a Linux container image.&lt;/p&gt; &lt;p&gt;Usually, a Linux container image is hosted in an image repository such as &lt;a href="https://quay.io"&gt;Red Hat Quay.io&lt;/a&gt;. The container image can be targeted to any destination from that image repository, so a variety of applications can use the image. This is all possible because the microservice is encapsulated into a discrete deployment unit that can be transported to any destination. The encapsulation removes from developers all tasks except configuration and deployment.&lt;/p&gt; &lt;p&gt;Transportable microservices also make them easier to use in an automated or declarative deployment process.&lt;/p&gt; &lt;h3&gt;4. A microservice carries its own data&lt;/h3&gt; &lt;p&gt;A microservice should have its own data storage mechanism that is isolated from all other microservices. The only way data can be shared with other microservice is by way of a public interface that the microservice publishes.&lt;/p&gt; &lt;p&gt;This principle imposes some discipline on data sharing: For instance, the particular data schema used by each microservice has to be well-documented. The design rules out behind-the-scenes hanky-panky that makes data hard to access or understand.&lt;/p&gt; &lt;p&gt;The principle that a microservice carries its own data is hard for many developers to accept. The common argument against a microservice carrying its own data is that the principle leads to a proliferation of data redundancy.&lt;/p&gt; &lt;p&gt;For example, imagine an e-commerce application. That application might have a microservice that manages customer profile information. The application has another microservice that handles purchases. When the principle that every microservice carries its own data is in force, it's quite possible that the purchases microservice might have data that is redundant with the customer profile microservice. Such data redundancy goes against the grain of developers who embrace the DRY principle (Don't Repeat Yourself).&lt;/p&gt; &lt;p&gt;On the other hand, developers who embrace the carry-their-own data principle understand the benefits and have adjusted accordingly. When a microservice carries its own data, any strange behavior is confined within the microservice.&lt;/p&gt; &lt;p&gt;When microservices try to share data, one microservice can make a change that causes a side effect in another microservice. This is fundamentally a bad way of doing business.&lt;/p&gt; &lt;p&gt;One of the key benefits of a microservice carrying its own data is that it enforces all of the other principles. This is particularly important when it comes to the final principle: That a microservice is ephemeral.&lt;/p&gt; &lt;h3&gt;5. A microservice is ephemeral&lt;/h3&gt; &lt;p&gt;The principle that a microservice is ephemeral means that it can be created, destroyed, and replenished on demand on a given target easily, quickly, and with no side effects. The standard operating expectation is that microservices come and go all the time, sometimes due to system failure and sometimes due to scaling demands.&lt;/p&gt; &lt;p&gt;This scenario is common in a &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; environment that uses the &lt;a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"&gt;Horizontal Pod Autoscaler&lt;/a&gt; (HPA) to accommodate scaling demands. The HPA creates and destroys containers according to momentary demands. Each time a container is created, an IP address is assigned dynamically. There are even situations where port numbers will be assigned dynamically too. Such are the impacts of ephemeral computing.&lt;/p&gt; &lt;p&gt;As a result, this principle that a microservice is ephemeral has two basic implications. The first is that developers need to create microservices that are good citizens in the application domain. This means implementing graceful startup and shutdown behavior within the microservice.&lt;/p&gt; &lt;p&gt;The second implication is that when programming their microservices, developers rely on runtime configuration settings to define external dependencies. This hand-off to an external configuration differs greatly from creating a monolithic application, where most dependencies are known at design time and are baked into the code. In microservice development, they're not. Instead, developers rely upon dynamic configuration to establish dependencies that are both internal and external to the microservice.&lt;/p&gt; &lt;p&gt;As strange as it might sound, coding to an ephemeral environment requires developers to accept that there are "known unknowns" that will be apparent at runtime. Hence, the need to program accordingly.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;The next article in this series will show how some companies incrementally move from a monolithic application to microservices. In the meantime, you can find more resources on our &lt;a href="https://developers.redhat.com/topics/microservices"&gt;microservices topic page&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/01/11/5-design-principles-microservices" title="5 design principles for microservices"&gt;5 design principles for microservices&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Bob Reselman</dc:creator><dc:date>2022-01-11T07:00:00Z</dc:date></entry><entry><title type="html">Announcing DashBuilder 0.14.1</title><link rel="alternate" href="https://blog.kie.org/2022/01/announcing-dashbuilder-0-14-1.html" /><author><name>William Siqueira</name></author><id>https://blog.kie.org/2022/01/announcing-dashbuilder-0-14-1.html</id><updated>2022-01-10T21:04:11Z</updated><content type="html">We are glad to announce that we are releasing DashBuilder 0.14.1! The major change for this new release is the adoption of Quarkus as the backend for Dashbuilder Runtime and the introduction of Dashbuilder Authoring, a new tool to create dashboards. The project code has also moved to Kogito tooling, which means that it will be regularly released with other kogito tooling tools. You may be asking why 0.14.1 and not 8.0. The reason is that Dashbuilder is now part of  and will follow its release cycle. DASHBUILDER IMPROVEMENTS AND NEW FEATURES The major change for 0.14.1 is that now Dashbuilder Runtime is based on Quarkus. It means quicker startup time (10x faster to start) and better overall performance. As a consequence we now have some changes: * Dev mode now uses SSE (Server-Sent Event); * Client to server communication is done via REST; * HTTP Compression comes from Quarkus; * New uploaded can be disabled using system property dashbuilder.runtime.allowUpload; When false new uploads will not be supported; * SQL DataSource configuration can be done using system properties: java -Ddashbuilder.datasources=sample \ -Ddashbuilder.datasource.sample.jdbcUrl=jdbc:mariadb://localhost:3306/sample \ -Ddashbuilder.datasource.sample.providerClassName=org.mariadb.jdbc.Driver \ -Ddashbuilder.datasource.sample.maxSize=10 \ -Ddashbuilder.datasource.sample.principal=repasse \ -Ddashbuilder.datasource.sample.credential=repasse \ -jar target/dashbuilder-runtime-8.0.0-Alpha.jar Make sure to change JDBC URL and Driver according to the used DB. The supported databases are the same as supported by . * Bean datasets now work by simply adding the JAR with the bean dataset and all its dependencies to the classpath; All features and dashboards that used to work on 7.x should work on 8.x with the exception of ElasticSearch, which was removed in this new version. To author dashboards we are releasing Dashbuilder Authoring, which we will talk more about later in this post. RUNNING DASHBUILDER RUNTIME 0.14.1 Download the distribution JAR and run it using Java 11 or later with the java -jar command. java -jar dashbuilder-runtime-app.jar The bootstrap switches for the 7.x continuous working with this new version. For example, this is how you run with a static dashboard: java -Ddashbuilder.runtime.import=/path/to/dashboard.zip -jar dashbuilder-runtime-app.jar Or in multi mode: java -Ddashbuilder.runtime.multi=true -jar dashbuilder-runtime-app.jar DASHBUILDER AUTHORING We are now releasing DashBuilder Authoring, a tool to author dashboards! Datasets, pages and navigation is saved directly in the filesystem. The path to the saved artifacts can be configured using system property org.dashbuilder.project.location (default is ./dashbuilder) and you can also automatically export the dashboard by setting a path to the exported ZIP using the system property dashbuilder.export.location. DashBuilder Authoring is distributed in the form of bootable JAR or a WAR that can be deployed on Wildfly 23.0.2. Deploy the WAR must be used when use of SQL dataset is required. The bootable JAR can be executed using Java 11+: java -jar dashbuilder-authoring-bootable.jar CONTAINER IMAGES It is possible to download and run dashbuilder binaries locally or you can try DashBuilder now using the following container images: Runtime quay.io/kogito_tooling_bot/dashbuilder-runtime You can pass Java properties using the JAVA_OPTS environment variable. For example, to run Dashbuilder Runtime in multi mode use the following command: podman run -p 8080:8080 -e “JAVA_OPTS=-Ddashbuilder.runtime.multi=true -Ddashbuilder.runtime.allowUpload=true” -dti quay.io/kogito_tooling_bot/dashbuilder-runtime:0.14.1 Authoring quay.io/kogito_tooling_bot/dashbuilder-authoring Dashbuilder Authoring uses the WAR bits and uses as base Wildfly 23.0.2 image. EXTERNAL DATASETS This release also includes a new dataset provider type called “External DataSets”. External Datasets are based in JSON, basically any JSON regular 2×2 array can be a dataset now. More information can be found in , but this will be covered later in its own post. CONCLUSION We introduced the new DashBuilder! In the next post we will share a new repository of sample dashboards that you can run with DashBuilder Runtime, so stay tuned! The post appeared first on .</content><dc:creator>William Siqueira</dc:creator></entry></feed>
