<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0" uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>A developer's guide to CI/CD and GitOps with Jenkins Pipelines</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/01/13/developers-guide-cicd-and-gitops-jenkins-pipelines" /><author><name>Bob Reselman</name></author><id>19d5abe2-3efb-49c1-91ad-8752f9686426</id><updated>2022-01-13T07:00:00Z</updated><published>2022-01-13T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/topics/ci-cd"&gt;CI/CD&lt;/a&gt;, or continuous integration and continuous delivery, is an essential part of the modern software development life cycle. Coupled with &lt;a href="https://developers.redhat.com/topics/gitops"&gt;GitOps&lt;/a&gt;, CI/CD allows developers to release high-quality software almost as soon as they commit code to a repository such as GitHub.&lt;/p&gt; &lt;p&gt;Automation is a key factor for implementing effective CI/CD. In this process, developers and release engineers create scripts that have all the instructions needed to test the code in a source code repository before putting it into a production environment. The process is efficient but complex. Fortunately, there are many tools that lessen the burden.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.jenkins.io"&gt;Jenkins&lt;/a&gt; is one of the most popular tools used for CI/CD. Jenkins has been around for years and has undergone numerous revisions, adding features all along the way. One of the most transformative features added to Jenkins is the ability to run &lt;a href="https://www.jenkins.io/doc/book/pipeline/"&gt;Jenkins Pipeline jobs&lt;/a&gt; driven by an automation script stored in a Jenkinsfile. Developers and release engineers can use Jenkinsfiles to combine the practices of CI/CD and GitOps into a unified deployment process. That's the focus of this article.&lt;/p&gt; &lt;p&gt;We'll start with a brief refresher of what Jenkins is and how it applies to both CI/CD and GitOps. Then, I’ll guide you through how to use a Jenkinsfile to create deployments that combine CI/CD and GitOps.&lt;/p&gt; &lt;h2&gt;How Jenkins supports CI/CD&lt;/h2&gt; &lt;p&gt;Jenkins is an open source tool for managing deployment processes, which can range from a single task—such as running a unit test against source code—to a complex deployment process embodying many tasks. From its first release, Jenkins allowed companies to standardize their deployment process: Once a job was configured on the Jenkins server, that job could run repeatedly in the same manner according to its configuration. The developer defined the tasks to run and when to run them, and Jenkins did the rest.&lt;/p&gt; &lt;p&gt;Early releases of Jenkins required developers to define their deployment processes manually, using the Jenkins Dashboard. Moreover, each job was specific to the particular Jenkins server. Deployments (&lt;em&gt;aka&lt;/em&gt; jobs) were not easy to update or transfer among servers. If developers wanted to update a particular job, they had to go to the server’s Jenkins Dashboard and manually implement the update. And, if developers or sysadmins wanted to move a job to another Jenkins server, they had to get into the file system of the Jenkins server and copy particular directories to other target Jenkins servers. The process was laborious, particularly if the job in question was large and contained many details.&lt;/p&gt; &lt;p&gt;Fortunately, the new Jenkins Pipeline job feature addresses these drawbacks head-on.&lt;/p&gt; &lt;h2&gt;Integrating CI/CD and GitOps with Jenkinsfiles&lt;/h2&gt; &lt;p&gt;A &lt;em&gt;Jenkinsfile&lt;/em&gt; is a text file, written in the &lt;a href="https://groovy-lang.org/"&gt;Groovy&lt;/a&gt; programming language, that defines each step in a Jenkins job. Usually, a Jenkinsfile is created by a developer or system administrator with a detailed understanding of how to deploy the particular component or application.&lt;/p&gt; &lt;p&gt;Once a Jenkinsfile is created, it is committed to a repository in the version control system that’s hosting the source code. After the Jenkinsfile is committed, the developer or sysadmin creates a job in Jenkins that declares the location of the Jenkinsfile in the source code repository and instructs Jenkins when to execute the Jenkinsfile. That’s it. There is no extensive twiddling with configuration settings in the user interface (UI). The Jenkinsfile has all the instructions required to run the job. (See Figure 1.)&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/file.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/file.png?itok=4mn91ZA_" width="883" height="307" alt="The Jenkinsfile describing an application’s deployment process is imported from a source control management system and executed by the Jenkins server." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1. The Jenkinsfile is imported from a source control management system (SCM) and executed by the Jenkins server. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Using a Jenkinsfile makes transferring jobs between servers much easier. All that’s required is to spin up a new job in Jenkins, bind that job to the Jenkinsfile that’s stored in version control, and then declare when the job is supposed to run. All the details and intelligence about the deployment are defined in the Jenkinsfile.&lt;/p&gt; &lt;p&gt;For all intents and purposes, the Jenkinsfile is the single source of truth (SSOT) about how a deployment executes. And that SSOT is hosted in the version control repository. Putting the SSOT in a version control repository is emblematic of the GitOps way of doing things, so let's talk about that next.&lt;/p&gt; &lt;h2&gt;GitOps and the single source of truth&lt;/h2&gt; &lt;p&gt;In a GitOps-driven deployment process, all activities emanate from the version-controlled code repository. Some companies drive their GitOps deployment process directly within the particular code repository service, such as GitHub, BitBucket, or Gitlab. Other companies have an external agent such as Jenkins execute the deployment process.&lt;/p&gt; &lt;p&gt;Regardless of the approach you choose, the important thing to understand about GitOps is that the single source of truth for all deployment activity is the code repository. Using a Jenkinsfile that’s hosted in a repository to define a job that runs under Jenkins fits well with the GitOps sensibility.&lt;/p&gt; &lt;p&gt;Now that we’ve covered how Jenkins implements CI/CD, and how it fits into the GitOps way of doing things, let’s move to a concrete example. Over the next few sections, we will implement a CI/CD process by running a Jenkinsfile under a Jenkins Pipeline job.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: Are you curious about how &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt; and Jenkins Pipelines work together? We recommend &lt;a href="https://cloud.redhat.com/blog/jenkins-pipelines"&gt;Simply Explained: OpenShift and Jenkins Pipelines&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Deploy a Node.js application using a Jenkinsfile&lt;/h2&gt; &lt;p&gt;Recall that a Jenkinsfile is a text file that describes the details of a job that will run under Jenkins. This section presents a job that executes a three-stage deployment to build, test, and release a &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js&lt;/a&gt; application:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;The build stage gets the application source code from GitHub and installs the dependency packages.&lt;/li&gt; &lt;li&gt;The test stage tests the application.&lt;/li&gt; &lt;li&gt;The release stage encapsulates the application into a Docker image that is then stored in a local container repository.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The following Jenkinsfile does the work of creating the local container repository. Once the container image is stored in the container repository, the Jenkinsfile runs a Docker container from the stored container image:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;node { env.NODEJS_HOME = "${tool 'basic_node'}" // on linux / mac env.PATH="${env.NODEJS_HOME}/bin:${env.PATH}" sh 'npm --version' } pipeline { agent any stages { stage('build') { steps { git branch: 'main', url: 'https://github.com/reselbob/secret-agent.git' sh "npm install" } } stage('test') { steps { script { env.SECRET_AGENT_PORT = "3060" echo "SECRET_AGENT_PORT is '${SECRET_AGENT_PORT}'" } sh "npm test" } } stage('release') { steps { script { env.SECRET_AGENT_PORT = "3050" echo "SECRET_AGENT_PORT is '${SECRET_AGENT_PORT}'" } // If the local registry container does not exists, create it sh """ if ! [ \$(docker ps --format '{{.Names}}' | grep -w registry &amp;&gt; /dev/null) ]; then docker run -d --network='host' -p 5000:5000 --restart=always --name registry registry:2; fi; """ // if the secret_agent container is running, delete it in order to create a new one sh """ if [ \$(docker ps --format '{{.Names}}' | grep -w secret_agent &amp;&gt; /dev/null) ]; then docker rm -f secret_agent; fi; """ sh "docker build -t secretagent:v1 . " sh "docker tag secretagent:v1 localhost:5000/secretagent:v1 " sh "docker run -d --network='host' -p 3050:3050 --name secret_agent localhost:5000/secretagent:v1 " sh "echo 'Secret Agent up and running on port 3050' " } } } }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note that the Jenkinsfile has two root-level sections: &lt;code&gt;node&lt;/code&gt; and &lt;code&gt;pipeline&lt;/code&gt;. We'll look at these next.&lt;/p&gt; &lt;h3&gt;The node section of the Jenkinsfile&lt;/h3&gt; &lt;p&gt;The &lt;code&gt;node&lt;/code&gt; section is the first step in the deployment process. It establishes a workspace in the Jenkins server under which a deployment runs. The workspace runs a Node.js application.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; The word &lt;em&gt;node&lt;/em&gt; has two different meanings here. Used by itself in a Jenkinsfile, &lt;code&gt;node&lt;/code&gt; describes the workspace. When used in the variable &lt;code&gt;basic_node&lt;/code&gt;, it refers to the Node.js runtime environment.&lt;/p&gt; &lt;p&gt;In this example of the &lt;code&gt;node&lt;/code&gt; section, the Jenkinsfile adds the location of the Node.js package to the environment’s PATH and then executes &lt;code&gt;npm --version&lt;/code&gt; to verify that the Node.js package manager is installed and accessible globally. By implication, a successful &lt;code&gt;npm --version&lt;/code&gt; command demonstrates that Node.js is installed.&lt;/p&gt; &lt;p&gt;The &lt;code&gt;npm --version&lt;/code&gt; command is executed as a parameter to the &lt;code&gt;sh&lt;/code&gt; command. The &lt;code&gt;sh&lt;/code&gt; command is used in the Jenkinsfile to execute commands a developer typically runs in a terminal window at the command line.&lt;/p&gt; &lt;h3&gt;The pipeline section of the Jenkinsfile&lt;/h3&gt; &lt;p&gt;The &lt;code&gt;pipeline&lt;/code&gt; section that follows the &lt;code&gt;node&lt;/code&gt; section defines the deployment that executes in three stages: build, test, and release. Each stage has a &lt;code&gt;steps&lt;/code&gt; subsection that describes the tasks that need to be executed in that stage.&lt;/p&gt; &lt;h4&gt;Build, test, and release&lt;/h4&gt; &lt;p&gt;The &lt;code&gt;build&lt;/code&gt; stage clones the application from a source code repository and installs the Node.js dependency packages that the application requires.&lt;/p&gt; &lt;p&gt;The &lt;code&gt;test&lt;/code&gt; stage executes a &lt;code&gt;sh&lt;/code&gt; command that, in turn, runs a &lt;code&gt;npm test&lt;/code&gt; command that is special to the application. Executing &lt;code&gt;npm test&lt;/code&gt; runs the various tests that are defined within the Node.js application’s source code.&lt;/p&gt; &lt;p&gt;The instructions for the release stage are a bit more complicated. The first thing the release stage does is check whether the local container registry is running. If the local registry does not exist, the Jenkinsfile creates it.&lt;/p&gt; &lt;h4&gt;Conditional commands&lt;/h4&gt; &lt;p&gt;The next set of commands checks whether the container that will be created from the source code is already running. The container’s name is &lt;code&gt;secret_agent&lt;/code&gt;. If the container is running, the Jenkinsfile deletes it. This is done so that the Jenkins job can run repeatedly. If a second run of the Jenkins job were to encounter a running instance of the &lt;code&gt;secret_agent&lt;/code&gt; container, the job would fail. Thus, any existing &lt;code&gt;secret_agent&lt;/code&gt; container needs to be deleted.&lt;/p&gt; &lt;p&gt;Once all the conditional commands have been executed, the Jenkinsfile builds a Docker image for the &lt;code&gt;secret_agent&lt;/code&gt; code and pushes the image into the local registry. Then, an instance of the &lt;code&gt;secret_agent&lt;/code&gt; container is created using the image stored in the local container registry.&lt;/p&gt; &lt;p&gt;The important thing to note about the deployment process is that all the instructions relevant to running a job under Jenkins are defined in the Jenkinsfile. If you ever need to change any deployment instructions, you don’t need to fiddle around with the Jenkins UI. All you need to do is alter the Jenkinsfile. Isolating this work to the Jenkinsfile makes it easier to manage and audit changes to the deployment process.&lt;/p&gt; &lt;p&gt;Now we've covered the Jenkinsfile and its sections. The last thing you need to do is bind the Jenkinsfile to a Jenkins job. This is done from within the Jenkins UI, as described in the next section.&lt;/p&gt; &lt;h2&gt;Binding a Jenkinsfile to a Jenkins Pipeline job&lt;/h2&gt; &lt;p&gt;Binding a Jenkinsfile to a Jenkins Pipeline job is straightforward, as shown by the sequence of screens in Figure 2. Here are the steps:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Create a job in the Jenkins Dashboard.&lt;/li&gt; &lt;li&gt;Name the job.&lt;/li&gt; &lt;li&gt;Declare it a Jenkins Pipeline job.&lt;/li&gt; &lt;/ol&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/project.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/project.png?itok=OKu71eA5" width="1125" height="684" alt="On the Jenkins Dashboard, you create and name a job and mark it as a Jenkins Pipeline." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2. On the Jenkins Dashboard, you create and name a job and mark it as a Jenkins Pipeline. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;Once the Jenkins Pipeline job is created, it needs to be configured. Figure 3 shows the first two steps in the process:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Enter a description for the job.&lt;/li&gt; &lt;li&gt;Declare how often to check the source code repository for changes.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;In Figure 3, the job is configured to poll the source code repository every 15 minutes.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/config_0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/config_0.png?itok=8N-WC_CY" width="1068" height="569" alt="After entering a description, choose a build trigger." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3. After entering a description, choose a build trigger. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Once the description and polling interval are declared, complete the configuration that binds the Jenkinsfile to the job. The following steps are illustrated in Figure 4:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Select &lt;strong&gt;Pipeline script from SCM&lt;/strong&gt; from the first dropdown menu.&lt;/li&gt; &lt;li&gt;Select &lt;strong&gt;Git&lt;/strong&gt; from the next dropdown menu. Doing this reveals an additional set of text boxes where you can do the following: &lt;ul&gt;&lt;li&gt;Enter the URL of the version code repository that has the Jenkinsfile.&lt;/li&gt; &lt;li&gt;Enter the branch that has the version of the Jenkinsfile of interest.&lt;/li&gt; &lt;li&gt;Declare the name of the Jenkinsfile.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;Figure 4 uses the default file, simply named &lt;code&gt;Jenkinsfile&lt;/code&gt;.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/job.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/job.png?itok=YTb8ARTK" width="973" height="688" alt="Configure the Jenkins Pipeline job with the URL and branch of the repository and the the name of the Jenkinsfile." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4. Configure the Jenkins Pipeline job with the URL and branch of the repository and the name of the Jenkinsfile. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;You can have many alternative Jenkinsfiles, with names such as &lt;code&gt;Jenkinsfile_k8s&lt;/code&gt; or &lt;code&gt;Jenkinsfile_windows&lt;/code&gt;. This means that you can use the same source code repository for a variety of jobs. Each job will execute its own build instructions described by the relevant Jenkinsfile.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; The versatility that comes with using many different versions of Jenkinsfiles is particularly useful for working in application environments that have complex provisioning and configuration requirements. For example, &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; and OpenShift configurations support a wide variety of settings—everything from container configuration to security rules. Putting the provisioning and configuration settings in one Jenkinsfile can become a maintenance headache. But, splitting up configurations among many files in a way that is particular to each environment makes deployment management a lot easier. The savings in labor alone can be significant.&lt;/p&gt; &lt;p&gt;Once the Jenkins Pipeline job is configured, you can run it from the Jenkins Dashboard. As shown in Figure 5, select the Jenkins Pipeline job, then click the &lt;strong&gt;Build Now&lt;/strong&gt; menu button to run it. The job runs and the result of each stage is shown in the job’s page on the Jenkins Dashboard.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/result.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/result.png?itok=rtSL3o7z" width="1315" height="812" alt="The results of executing a Jenkinsfile are shown in the job's Jenkins Dashboard page." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 5. The results of executing a Jenkinsfile are shown in the job's Jenkins Dashboard page. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The beauty of it all is that the job's runtime details are described in the associated Jenkinsfile. Should the deployment process need to change, all you need to do is change the Jenkinsfile in the source code repository.&lt;/p&gt; &lt;h2&gt;The benefits of using Jenkins Pipelines for GitOps and CI/CD&lt;/h2&gt; &lt;p&gt;There are many benefits to using Jenkins Pipeline jobs in conjunction with a version control server such as GitHub, and thus combining CI/CD and GitOps.&lt;/p&gt; &lt;p&gt;First, putting deployment instructions in a central version control repository makes the repository a central location, which can be used as a single source of truth for all deployment activities. Using the repository as a single source of truth provides reliable change management and auditing capabilities. Version control and access security are also built into the service.&lt;/p&gt; &lt;p&gt;Second, declaring the build process in a Jenkinsfile makes it easier to automate deployment. You don’t have to fiddle with a UI to get your programs out; you can just write the code and let the Jenkins Pipeline do the rest.&lt;/p&gt; &lt;p&gt;Finally, Jenkins is a well-known CI/CD tool. It has all the features that are required for implementing a viable GitOps-focused deployment process. For companies already using Jenkins, making the leap to GitOps using a Jenkinsfile is much easier than starting from scratch. And for companies not yet using Jenkins, the learning curve is acceptable. This technology has proven itself over the years, and there are many &lt;a href="https://developers.redhat.com/courses/gitops"&gt;learning resources and examples&lt;/a&gt; developers can use to get up to speed.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/topics/gitops"&gt;GitOps&lt;/a&gt; has a lot to offer developers who want to automate their deployment processes using scripts that are stored as a single source of truth in a version control management system. Such control and reliability are compelling reasons to make the move.&lt;/p&gt; &lt;p&gt;You can't adopt GitOps in a day or two. It takes time to get the organizational processes in place. But many developers and their organizations will find that the time required to get GitOps working under a Jenkins Pipeline job, using a Jenkinsfile, is a good investment, especially for the benefits at hand.&lt;/p&gt; &lt;h2&gt;Learn more about GitOps and CI/CD on Red Hat Developer&lt;/h2&gt; &lt;p&gt;Visit the following articles and resources for more about using GitOps and CI/CD for secure, automated deployments:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Red Hat Developer learning courses: &lt;a href="https://developers.redhat.com/courses/gitops"&gt;Develop with GitOps&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/blog/2021/05/13/why-should-developers-care-about-gitops"&gt;Why should developers care about GitOps?&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/06/29/how-apply-machine-learning-gitops"&gt;How to apply machine learning to GitOps&lt;/a&gt;&lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/articles/2021/08/03/managing-gitops-control-planes-secure-gitops-practices"&gt;Managing GitOps control planes for secure GitOps practices&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/articles/2021/06/21/modern-fortune-teller-using-gitops-automate-application-deployment-red-hat"&gt;Modern Fortune Teller: Using GitOps to automate application deployment on Red Hat OpenShift&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/01/13/developers-guide-cicd-and-gitops-jenkins-pipelines" title="A developer's guide to CI/CD and GitOps with Jenkins Pipelines"&gt;A developer's guide to CI/CD and GitOps with Jenkins Pipelines&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Bob Reselman</dc:creator><dc:date>2022-01-13T07:00:00Z</dc:date></entry><entry><title type="html">This Week in JBoss - January 13th 2022</title><link rel="alternate" href="https://www.jboss.org/posts/weekly-2022-01-13.html" /><category term="quarkus" /><category term="java" /><category term="resteasy" /><category term="camel" /><category term="reactive" /><category term="panache" /><category term="hibernate" /><category term="event-driven" /><category term="netty" /><category term="kubernetes" /><author><name>Stefan Sitani</name><uri>https://www.jboss.org/people/stefan-sitani</uri><email>do-not-reply@jboss.com</email></author><id>https://www.jboss.org/posts/weekly-2022-01-13.html</id><updated>2022-01-13T00:00:00Z</updated><content type="html">&lt;article class="" data-tags="quarkus, java, resteasy, camel, reactive, panache, hibernate, event-driven, netty, kubernetes"&gt; &lt;h1&gt;This Week in JBoss - January 13th 2022&lt;/h1&gt; &lt;p class="preamble"&gt;&lt;/p&gt;&lt;p&gt;Happy new year, everyone! Welcome to the first JBoss weekly editorial of 2022. Enjoy our pick of the latest news and interesting reads from around the JBoss community.&lt;/p&gt;&lt;p&gt;&lt;/p&gt; &lt;div class="sect1"&gt; &lt;h2 id="_releases_releases_releases"&gt;Releases, releases, releases!&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;Here are the releases from the JBoss Community for this edition:&lt;/p&gt; &lt;div class="ulist square"&gt; &lt;ul class="square"&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://quarkus.io/blog/quarkus-2-6-0-final-released/"&gt;Quarkus 2.6.0&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://camel.apache.org/blog/2021/12/camel-quarkus-release-2.6.0/"&gt;Camel Quarkus 2.6.0&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://debezium.io/blog/2021/12/16/debezium-1.8-final-released/"&gt;Debezium 1.8.0&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://vertx.io/blog/eclipse-vert-x-3-9-12/"&gt;Eclipse Vert.x 3.9.12&lt;/a&gt; and &lt;a href="https://vertx.io/blog/eclipse-vert-x-4-2-3/"&gt;4.2.3&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://in.relation.to/2022/01/05/hibernate-search-6-0-8-Final/"&gt;Hibernate Search 6.0.8&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_articles_books_and_tutorials"&gt;Articles, Books, and Tutorials&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;New year, new content! Check out some of the recent articles about Java, Kubernetes, Quarkus, and more…​&lt;/p&gt; &lt;div class="sect2"&gt; &lt;h3 id="_new_book_reactive_systems_in_java"&gt;New Book: Reactive Systems in Java&lt;/h3&gt; &lt;p&gt;&lt;a href="https://www.oreilly.com/library/view/reactive-systems-in/9781492091714/"&gt;Reactive Systems in Java&lt;/a&gt;, by Clement Escoffier and Ken Finnigan&lt;/p&gt; &lt;p&gt;In their new book &lt;em&gt;Reactive Systems in Java&lt;/em&gt;, Clement Escoffier and Ken Finnigan take an in-depth look at reactive systems design make applications responsive, elastic, and resilient, and explore event-driven architectures as a flexible and composable option for designing distributed systems. Full of practical examples, their new book helps developers bring these approaches together when designing applications with Quarkus.&lt;/p&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_simplify_java_persistence_using_quarkus_and_hibernate_reactive"&gt;Simplify Java persistence using Quarkus and Hibernate Reactive&lt;/h3&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/articles/2022/01/06/simplify-java-persistence-using-quarkus-and-hibernate-reactive#"&gt;Simplify Java persistence using Quarkus and Hibernate Reactive&lt;/a&gt; by Daniel Oh&lt;/p&gt; &lt;p&gt;Daniel Oh walks you through developing a reactive application with persistent data storage that leverages the benefits of simplified JPA implementation using the new extension for Hibernate Reactive with Panache.&lt;/p&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_getting_started_with_netty"&gt;Getting started with Netty&lt;/h3&gt; &lt;p&gt;&lt;a href="http://www.mastertheboss.com/jboss-frameworks/netty/jboss-netty-tutorial/?utm_source=rss&amp;#38;utm_medium=rss&amp;#38;utm_campaign=jboss-netty-tutorial"&gt;Getting started with Netty&lt;/a&gt; by Francesco Marchioni&lt;/p&gt; &lt;p&gt;Francesco’s tutorial introduces you to Netty a client/server framework that provides a simplified layer over non-blocking I/O networking. The introduction briefly describes the notable features that Netty provides. The first part of the tutorial section explains how you can create a simple echo server. The second part shows you how you can expand it by adding a separate Netty Client with its own Handler, and also demonstrates how to send a String between the client and the server over the event loop.&lt;/p&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_why_you_should_migrate_your_java_workloads_to_openshift"&gt;Why you should migrate your Java workloads to OpenShift&lt;/h3&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/articles/2022/01/07/why-you-should-migrate-your-java-workloads-openshift#"&gt;Why you should migrate your Java workloads to OpenShift&lt;/a&gt; by Philip Hayes&lt;/p&gt; &lt;p&gt;In the first part of his two-part post series, Philip Hayes outlines the main benefits of migrating Java workloads to OpenShift and explores a number of tools available from Red Hat and from the Konveyor community project that are designed to help you transitions your Java workloads to the cloud with ease. Stay tuned for part 2 that will take an in-depth look at migrating a JBoss EAP application to OpenShift!&lt;/p&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_how_to_use_quarkus_with_the_service_binding_operator"&gt;How to use Quarkus with the Service Binding Operator&lt;/h3&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/articles/2021/12/22/how-use-quarkus-service-binding-operator"&gt;How to use Quarkus with the Service Binding Operator&lt;/a&gt; by Ioannis Kanellos&lt;/p&gt; &lt;p&gt;Ioannis Kanellos opens his article with a brief history of service binding solutions for Kubernetes developers and follows it up with a demonstration of a developer workflow centered around binding an external database service to an application in a cloud environment. With plenty of code examples and technical detail, Ioannis walks you through provisioning a database service, writing a Quarkus app for simplified data access with Hibernate with Panache, generating the service binding resources using the Service Binding Operator, and deploying your project to a containerized Kubernetes cluster running on your local machine.&lt;/p&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_videos"&gt;Videos&lt;/h3&gt; &lt;p&gt;Here’s my pick of this week’s YouTube videos:&lt;/p&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://youtu.be/1_GUWDeQIsc"&gt;Drag and Drop your Quarkus Serverless App on the Developer Sandbox&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://youtu.be/1Rt7CPFodbE"&gt;Quarkus Insights #75: State of the Quarkus Ecosystem&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;p&gt;&lt;em&gt;That’s all for today! Please join us again in two weeks for another round of our JBoss editorial!&lt;/em&gt;&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="author"&gt; &lt;pfe-avatar pfe-shape="circle" pfe-pattern="squares" pfe-src="/img/people/stefan-sitani.png"&gt;&lt;/pfe-avatar&gt; &lt;span&gt;Stefan Sitani&lt;/span&gt; &lt;/div&gt;&lt;/article&gt;</content><dc:creator>Stefan Sitani</dc:creator></entry><entry><title>Integrate Apache ActiveMQ brokers using Camel K</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/01/12/integrate-apache-activemq-brokers-using-camel-k" /><author><name>Maz Arslan, Anton Giertli</name></author><id>09666143-7ea3-489e-a486-4929d1682e5d</id><updated>2022-01-12T07:00:00Z</updated><published>2022-01-12T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://activemq.apache.org"&gt;Apache ActiveMQ&lt;/a&gt; is a highly popular message broker that features persistence, guaranteed message delivery, and high throughput. &lt;a href="https://activemq.apache.org/components/artemis/"&gt;Apache ActiveMQ Artemis&lt;/a&gt; streamlines the classic message broker implementation for &lt;a href="https://developers.redhat.com/topics/microservices"&gt;microservices&lt;/a&gt; architectures. This article is for developers transitioning from &lt;a href="https://activemq.apache.org/components/classic/"&gt;ActiveMQ Classic&lt;/a&gt; to &lt;a href="https://activemq.apache.org/components/artemis/"&gt;ActiveMQ Artemis&lt;/a&gt;. We will show you how to get the two versions working together using &lt;a href="https://camel.apache.org/camel-k/1.7.x/index.html"&gt;Apache Camel K&lt;/a&gt;. Our example is based on &lt;a href="https://developers.redhat.com/products/amq/overview"&gt;Red Hat AMQ&lt;/a&gt; versions 6 and 7, and we will perform the steps on &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift 4&lt;/a&gt;. Our code is written in &lt;a href="https://developers.redhat.com/enterprise-java"&gt;Java&lt;/a&gt;. The integration process and techniques should be applicable to many other scenarios.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; See &lt;a href="https://developers.redhat.com/articles/2021/06/30/implementing-apache-activemq-style-broker-meshes-apache-artemis"&gt;Implementing Apache ActiveMQ-style broker meshes with Apache Artemis&lt;/a&gt; for a discussion of the differences between Red Hat AMQ 6 and 7.&lt;/p&gt; &lt;h2&gt;The Camel K integration workflow&lt;/h2&gt; &lt;p&gt;Camel K is an integration framework that allows developers to exchange data easily between many common data processing tools. The demonstration in this article creates two ActiveMQ brokers, one using Red Hat AMQ 6 and the other using Red Hat AMQ 7. We then create two Camel K integrations: One to help AMQ 7 consume messages, and another to help AMQ 6 produce them. Finally, we create a third integration that functions as a message bridge between the two brokers.&lt;/p&gt; &lt;p&gt;Figure 1 illustrates the Camel K integration workflow.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/AMQSSL_CAMELK%20Diagram.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/AMQSSL_CAMELK%20Diagram.png?itok=ygU8KNpA" width="960" height="540" alt="Figure 1. A message bridge connects two ActiveMQ brokers." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1. A message bridge connects two ActiveMQ brokers.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;You can retrieve the files for the demonstration from the &lt;a href="https://github.com/MazArslan/CamelK-SSL-Demo"&gt;GitHub repository&lt;/a&gt; associated with this article.&lt;/p&gt; &lt;h2&gt;Prerequisites&lt;/h2&gt; &lt;p&gt;This article assumes you have a basic knowledge of the OpenShift &lt;code&gt;oc&lt;/code&gt; command-line interface and OpenShift web console. You need a running instance of OpenShift 4 and admin access on the OpenShift cluster. You will also need to &lt;a href="https://camel.apache.org/download/"&gt;install Camel K&lt;/a&gt; on your local system.&lt;/p&gt; &lt;h2&gt;Set up the cluster&lt;/h2&gt; &lt;p&gt;Create a namespace on your OpenShift instance:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc new-project camelk-ssl&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;As the cluster administrator, install the &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_integration/2020-q4/html/deploying_camel_k_integrations_on_openshift/installing-camel-k"&gt;Red Hat Camel K Operator&lt;/a&gt; and &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_amq/7.7/html-single/deploying_amq_broker_on_openshift/index"&gt;Red Hat AMQ 7.7 Operator&lt;/a&gt; on your OpenShift instance from the OpenShift OperatorHub.&lt;/p&gt; &lt;p&gt;Now, create certificates to enable secure SSL communication. Make sure you set the common name &lt;code&gt;CN&lt;/code&gt; on the broker to your server domain name:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt; export CLIENT_KEYSTORE_PASSWORD=password export CLIENT_TRUSTSTORE_PASSWORD=password export BROKER_KEYSTORE_PASSWORD=password export BROKER_TRUSTSTORE_PASSWORD=password #Client Keystore keytool -genkey -alias client -keyalg RSA -keystore client.ks -storepass $CLIENT_KEYSTORE_PASSWORD -keypass $CLIENT_KEYSTORE_PASSWORD -dname "CN=camelssl-example, O=RedHat, C=UK" #Broker Keystore keytool -genkey -alias broker -keyalg RSA -keystore broker.ks -storepass $BROKER_KEYSTORE_PASSWORD -keypass $BROKER_KEYSTORE_PASSWORD -dname "CN=*.apps.cluster-hhl6r.hhl6r.sandbox55.opentlc.com, O=RedHat, C=UK" #Export Client PublicKey keytool -export -alias client -keystore client.ks -storepass $CLIENT_KEYSTORE_PASSWORD -file client.cert #Export Server PublicKey keytool -export -alias broker -keystore broker.ks -storepass $BROKER_KEYSTORE_PASSWORD -file broker.cert #Import Server PublicKey into Client Truststore keytool -import -alias broker -keystore client.ts -file broker.cert -storepass $CLIENT_TRUSTSTORE_PASSWORD -trustcacerts -noprompt #Import Client PublicKey into Server Truststore keytool -import -alias client -keystore broker.ts -file client.cert -storepass $BROKER_TRUSTSTORE_PASSWORD -trustcacerts -noprompt #Import Server PublicKey into Server Truststore (i.e.: trusts its self) keytool -import -alias broker -keystore broker.ts -file broker.cert -storepass $BROKER_TRUSTSTORE_PASSWORD -trustcacerts -noprompt &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Set up the ActiveMQ consumer (AMQ 7)&lt;/h2&gt; &lt;p&gt;Create a secret for the broker keystore and truststore for AMQ 7:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc create secret generic example-amq-secret --from-file=broker.ks --from-literal=keyStorePassword=password --from-file=client.ts=broker.ts --from-literal=trustStorePassword=password&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Create an AMQ 7 broker using the AMQ broker operator:&lt;/p&gt; &lt;pre&gt; &lt;code&gt; apiVersion: broker.amq.io/v2alpha4 kind: ActiveMQArtemis metadata: name: example-amq application: example-amq namespace: camelk-ssl spec: deploymentPlan: size: 1 image: registry.redhat.io/amq7/amq-broker:7.6 requireLogin: false adminUser: admin adminPassword: admin console: expose: true acceptors: - name: amqp protocols: amqp port: 5672 sslEnabled: true sslSecret: example-amq-secret verifyHost: false expose: true &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Create a new AMQ broker address. This configuration also creates a route and service for the broker:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;apiVersion: broker.amq.io/v2alpha2 kind: ActiveMQArtemisAddress metadata: name: example-testaddress spec: addressName: test queueName: test routingType: anycast &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To test your AMQ 7 broker, you have to download &lt;a href="https://activemq.apache.org/components/artemis/download/"&gt;ActiveMQ Artemis&lt;/a&gt;. Once the download is completed, extract it and open the &lt;code&gt;/bin&lt;/code&gt; folder, where you will find the Apache Artemis script that is used for testing. You can produce and consume messages using the new AMQ 7 broker from your local machine; just make sure to change the URL and truststore location.&lt;/p&gt; &lt;p&gt;Here is a sample command to produce messages:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ./artemis producer --url 'amqps://example-amq7-amqp-0-svc-rte-camelk-ssl.apps.cluster-hhl6r.hhl6r.sandbox55.opentlc.com:443?jms.username=admin&amp;jms.password=admin&amp;transport.trustStoreLocation=/home/marslan/work/camelkssl/client.ts&amp;transport.trustStorePassword=password&amp;transport.verifyHost=false' --threads 1 --protocol amqp --message-count 10 --destination 'queue://test'&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Here is a sample command to consume messages:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$./artemis consumer --url 'amqps://example-amq7-amqp-0-svc-rte-camelk-ssl.apps.cluster-hhl6r.hhl6r.sandbox55.opentlc.com:443?jms.username=admin&amp;jms.password=admin&amp;transport.trustStoreLocation=/home/marslan/work/camelkssl/client.ts&amp;transport.trustStorePassword=password&amp;transport.verifyHost=false' --threads 1 --protocol amqp --message-count 10 --destination 'queue://test'&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Set up the ActiveMQ producer (AMQ 6)&lt;/h2&gt; &lt;p&gt;Next, create a secret for the broker keystore and truststore for AMQ 6:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc create secret generic example-amq6-secret --from-file=broker.ks --from-literal=keyStorePassword=password --from-file=broker.ts --from-literal=trustStorePassword=password &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Create an AMQ 6 broker from the command line:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc new-app amq63-ssl -p APPLICATION_NAME=amq6-broker -p MQ_QUEUES=test -p MQ_TOPICS=test -p MQ_USERNAME=admin \ -p MQ_PASSWORD=admin -p ActiveMQ_SECRET=example-amq6-secret -p AMQ_TRUSTSTORE=broker.ts -p AMQ_TRUSTSTORE_PASSWORD=password \ -p AMQ_KEYSTORE=broker.ks -p AMQ_KEYSTORE_PASSWORD=password -n camelk-ssl&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Create a route for the broker in the OpenShift console, using the &lt;code&gt;amq6-broker-tcp-ssl&lt;/code&gt; service.&lt;/p&gt; &lt;p&gt;To test your AMQ 6 broker, you can download &lt;a href="https://activemq.apache.org/components/classic/download/"&gt;ActiveMQ&lt;/a&gt;. Once the download is completed, extract it and open the &lt;code&gt;/bin&lt;/code&gt; folder, where you will find the ActiveMQ script that is used for testing. You can produce and consume messages using the new AMQ 6 broker from your local machine. Just make sure to change the URL and truststore location.&lt;/p&gt; &lt;p&gt;Enter the following command to produce messages:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ./activemq producer \ -Djavax.net.ssl.trustStore=/home/marslan/work/camelkssl/client.ts \ -Djavax.net.ssl.trustStorePassword=password \ -Djavax.net.ssl.keyStore=/home/marslan/work/camelkssl/client.ks \ -Djavax.net.ssl.keyStorePassword=password \ --brokerUrl ssl://test-camelk-ssl.apps.cluster-hhl6r.hhl6r.sandbox55.opentlc.com:443 \ --user admin \ --password admin \ --destination queue://test \ --messageCount 1000 \ --message HelloWorld &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Enter the following command to consume messages:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ./activemq consumer \ -Djavax.net.ssl.trustStore=/home/marslan/work/camelkssl/client.ts \ -Djavax.net.ssl.trustStorePassword=password \ -Djavax.net.ssl.keyStore=/home/marslan/work/camelkssl/client.ks \ -Djavax.net.ssl.keyStorePassword=password \ --brokerUrl ssl://test-camelk-ssl.apps.cluster-hhl6r.hhl6r.sandbox55.opentlc.com:443 \ --user admin \ --password admin \ --destination queue://test &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Test Camel K&lt;/h2&gt; &lt;p&gt;Now, we will import a simple Java application and use it to ensure that Camel K is working correctly. The &lt;code&gt;HelloCamelK.java&lt;/code&gt; program is available in the &lt;a href="https://github.com/MazArslan/CamelK-SSL-Demo"&gt;Camel K SSL Demo repository&lt;/a&gt; on GitHub.&lt;/p&gt; &lt;p&gt;Run the file with the following command (you may delete this integration after the test):&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ kamel run HelloCamelK&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Create the AMQ 7 consumer&lt;/h2&gt; &lt;p&gt;Create folders for AMQ 6 and AMQ 7:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ mkdir amq6 $ mkdir amq7&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Create a new configuration directory for AMQ 7:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ mkdir amq7/configs&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In the configuration directory, create an &lt;code&gt;application.properties&lt;/code&gt; file for the AMQ 7 integration, with the following contents:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;quarkus.qpid-jms.url=amqps://example-amq7-amqp-0-svc-rte-camelk-ssl.apps.cluster-hhl6r.hhl6r.sandbox55.opentlc.com:443?transport.keyStoreLocation=/etc/ssl/example-amq-secret/broker.ks&amp;transport.keyStorePassword=password&amp;transport.trustStoreLocation=/etc/ssl/example-amq-secret/client.ts&amp;transport.trustStorePassword=password&amp;transport.verifyHost=false quarkus.qpid-jms.username=admin quarkus.qpid-jms.password=admin jms.destinationType=queue jms.destinationName=test &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Within the &lt;code&gt;amq7&lt;/code&gt; folder, create a program named &lt;code&gt;amq7consumer.java&lt;/code&gt; for the AMQ 7 consumer integration:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;// camel-k: language=java &lt;1&gt; // camel-k: dependency=mvn:org.amqphub.quarkus:quarkus-qpid-jms &lt;2&gt; import org.apache.camel.builder.RouteBuilder; public class amq7consumer extends RouteBuilder { @Override public void configure() throws Exception { from("jms:{{jms.destinationType}}:{{jms.destinationName}}").to("log:info"); &lt;3&gt; } } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Let's look at a few parts of the code:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;camel-k: language=java&lt;/code&gt; provides information to Camel K to run the Java file. The comment enables Camel K to sort out dependencies.&lt;/li&gt; &lt;li&gt;&lt;code&gt;camel-k: dependency=mvn:org.amqphub.quarkus:quarkus-qpid-jms&lt;/code&gt; supports out-of-the-box configuration for SSL, without requiring any further customization. You can find more information in the &lt;a href="https://qpid.apache.org/releases/qpid-jms-0.54.0/docs/index.html#ssl-transport-configuration-options"&gt;Apache SSL documentation&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;The code consumes the &lt;code&gt;jms.url&lt;/code&gt;, &lt;code&gt;jms.destinationType&lt;/code&gt;, and &lt;code&gt;jms.destinationName&lt;/code&gt; properties specified in the &lt;code&gt;application.properties&lt;/code&gt; file and prints the values to the log.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The Java file is imported in the next step, where we run Camel K:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ kamel run -n camelk-ssl --property file:./amq7/configs/application.properties --resource secret:example-amq-secret@/etc/ssl/example-amq-secret amq7/amq7consumer.java &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In the &lt;code&gt;kamel run&lt;/code&gt; command:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;The &lt;code&gt;--property file&lt;/code&gt; option imports properties to Camel.&lt;/li&gt; &lt;li&gt;The &lt;code&gt;--resource&lt;/code&gt; option adds resources to the cluster pod. In this command, we are adding &lt;code&gt;example-amq-secret&lt;/code&gt; and placing its contents into the &lt;code&gt;/etc/ssl/example-amq-secret&lt;/code&gt; folder within the pod.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;To debug Camel K, add the following option to the &lt;code&gt;kamel run&lt;/code&gt; command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;--trait jvm.options=-Djavax.net.debug=all&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Create the AMQ 6 producer&lt;/h2&gt; &lt;p&gt;Create a new configuration directory for AMQ 6:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ mkdir amq6/configs&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In the configuration directory, create an &lt;code&gt;application.properties&lt;/code&gt; file for the AMQ 6 integration, with the following contents:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;activemq.destination.brokerURL=ssl://test-camelk-ssl.apps.cluster-hhl6r.hhl6r.sandbox55.opentlc.com:443 activemq.destination.username=admin activemq.destination.password=admin activemq.destination.ssl.keyStorePassword=password activemq.destination.ssl.keyStoreLocation=/etc/ssl/example-amq6-secret/broker.ks activemq.destination.ssl.trustStorePassword=password activemq.destination.ssl.trustStoreLocation=/etc/ssl/example-amq6-secret/broker.ts activemq.destination.type=queue activemq.destination.name=test &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Within the &lt;code&gt;amq6&lt;/code&gt; folder, create a program named &lt;code&gt;amq6SSLproducer.java&lt;/code&gt; for the AMQ 6 producer integration:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;// camel-k: dependency=camel:camel-quarkus-activemq // camel-k: dependency=camel:camel-quarkus-timer // camel-k: property=period=5000 public class amq6SSLProducer extends RouteBuilder { @Override public void configure() throws Exception { from("timer:foo?fixedRate=true&amp;period={{period}}").bean(this, "generateFakePerson()").to("log:info") .to("activemq:{{activemq.destination.type}}:{{activemq.destination.name}}?connectionFactory=#pooledConnectionFactory"); } public String generateFakePerson() { Faker faker = new Faker(); return faker.name().fullName() + " lives on " + faker.address().streetAddress(); } @ApplicationScoped public ActiveMQComponent activeMq(PooledConnectionFactory pooledConnectionFactory) { ActiveMQComponent activeMqComponent = new ActiveMQComponent(); activeMqComponent.setConnectionFactory(pooledConnectionFactory); activeMqComponent.setCacheLevelName("CACHE_CONSUMER"); return activeMqComponent; } @BindToRegistry public PooledConnectionFactory pooledConnectionFactory() throws Exception { return new PooledConnectionFactory(sslConnectionFactory()); } private ActiveMQSslConnectionFactory sslConnectionFactory() throws Exception { ActiveMQSslConnectionFactory connectionFactory = new ActiveMQSslConnectionFactory(); logger.info("BrokerURL: " + destinationBrokerURL); connectionFactory.setBrokerURL(destinationBrokerURL); connectionFactory.setUserName(destinationUserName); connectionFactory.setPassword(destinationPassword); connectionFactory.setTrustStore(destinationTrustStoreLocation); connectionFactory.setTrustStorePassword(destinationTruststorePassword); connectionFactory.setKeyStore(destinationKeyStoreLocation); connectionFactory.setKeyStorePassword(destinationKeystorePassword); return connectionFactory; } }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Run the producer using Camel K:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ kamel run -n camelk-ssl \ --property file:./amq6/configs/application.properties \ --resource secret:example-amq6-secret@/etc/ssl/example-amq6-secret amq6/amqssl.java&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Create a message bridge between AMQ 6 and AMQ 7&lt;/h2&gt; &lt;p&gt;Create new folders for the message bridge and its configuration:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ mkdir message-bridge $ mkdir message-bridge/configs&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In the configuration directory, create an &lt;code&gt;application.properties&lt;/code&gt; file for the message bridge, with the following contents:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;#amq6 activemq.source.brokerURL=ssl://test-camelk-ssl.apps.cluster-hhl6r.hhl6r.sandbox55.opentlc.com:443 activemq.source.username=admin activemq.source.password=admin activemq.source.ssl.keyStorePassword=password activemq.source.ssl.keyStoreLocation=/etc/ssl/example-amq6-secret/broker.ks activemq.source.ssl.trustStorePassword=password activemq.source.ssl.trustStoreLocation=/etc/ssl/example-amq6-secret/broker.ts activemq.source.type=queue activemq.source.name=test #amq7 quarkus.qpid-jms.url=amqps://example-amq7-amqp-0-svc-rte-camelk-ssl.apps.cluster-hhl6r.hhl6r.sandbox55.opentlc.com:443?transport.keyStoreLocation=/etc/ssl/example-amq-secret/broker.ks&amp;transport.keyStorePassword=password&amp;transport.trustStoreLocation=/etc/ssl/example-amq-secret/client.ts&amp;transport.trustStorePassword=password&amp;transport.verifyHost=false quarkus.qpid-jms.username=admin quarkus.qpid-jms.password=admin jms.destinationType=queue jms.destinationName=test&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In the &lt;code&gt;message-bridge&lt;/code&gt; folder, create a program named &lt;code&gt;sixToSevenBridge.java&lt;/code&gt;, with the following contents:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt; @Override public void configure() throws Exception { from("activeMQSource:{{activemq.source.type}}:{{activemq.source.name}}").to("log:info") .to("jms:{{jms.destinationType}}:{{jms.destinationName}}?connectionFactory=artemisConnectionFactory"); } @BindToRegistry("artemisConnectionFactory") public JmsConnectionFactory connectionFactory() throws Exception { return new JmsConnectionFactory(destinationBrokerURL); } @BindToRegistry("activeMQSource") public ActiveMQComponent activeMQSource() throws Exception{ ActiveMQComponent activeMqComponent = new ActiveMQComponent(); activeMqComponent.setConnectionFactory(pooledConnectionFactorySource()); activeMqComponent.setCacheLevelName("CACHE_CONSUMER"); return activeMqComponent; } @BindToRegistry("pooledConnectionFactorySource") public PooledConnectionFactory pooledConnectionFactorySource() throws Exception { return new PooledConnectionFactory(sslConnectionFactorySource()); } private ActiveMQSslConnectionFactory sslConnectionFactorySource() throws Exception { ActiveMQSslConnectionFactory connectionFactory = new ActiveMQSslConnectionFactory(); System.out.println("BrokerURL: " + sourceBrokerURL); connectionFactory.setBrokerURL(sourceBrokerURL); connectionFactory.setUserName(sourceUserName); connectionFactory.setPassword(sourcePassword); connectionFactory.setTrustStore(sourceTrustStoreLocation); connectionFactory.setTrustStorePassword(sourceTruststorePassword); connectionFactory.setKeyStore(sourceKeyStoreLocation); connectionFactory.setKeyStorePassword(sourceKeystorePassword); return connectionFactory; } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Run the message bridge:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ kamel run -n camelk-ssl \ --property file:./message-bridge/configs/application.properties message-bridge/sixToSevenBridge.java \ --resource secret:example-amq6-secret@/etc/ssl/example-amq6-secret \ --resource secret:example-amq-secret@/etc/ssl/example-amq-secret &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;View the logs to see whether the message bridge worked:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc logs -f {AMQ7 consumer pod} &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Clean up&lt;/h2&gt; &lt;p&gt;Uninstall the operators using the OpenShift command line:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc delete project camelk-ssl&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This article showed you how to use Camel K to connect different versions of the ActiveMQ message broker using Red Hat AMQ on OpenShift 4. Camel K is a highly effective integration framework that runs natively on &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt;. This article showcased just a few of its capabilities. Camel K allows you to build and &lt;a href="https://developers.redhat.com/topics/containers"&gt;containerize&lt;/a&gt; applications easily and quickly. Its integrations use very lightweight pods, thus consuming fewer resources.&lt;/p&gt; &lt;p&gt;Get more information from the &lt;a href="https://camel.apache.org/camel-k/next/"&gt;Camel K documentation&lt;/a&gt;. You might also enjoy the article &lt;a href="https://developers.redhat.com/blog/2020/05/12/six-reasons-to-love-camel-k"&gt;Six reasons to love Camel K&lt;/a&gt; and our series of related &lt;a href="https://developers.redhat.com/courses/camel-k"&gt;Camel K learning courses&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/01/12/integrate-apache-activemq-brokers-using-camel-k" title="Integrate Apache ActiveMQ brokers using Camel K"&gt;Integrate Apache ActiveMQ brokers using Camel K&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Maz Arslan, Anton Giertli</dc:creator><dc:date>2022-01-12T07:00:00Z</dc:date></entry><entry><title type="html">Kogito 1.15.0 released!</title><link rel="alternate" href="https://blog.kie.org/2022/01/kogito-1-15-0-released.html" /><author><name>Cristiano Nicolai</name></author><id>https://blog.kie.org/2022/01/kogito-1-15-0-released.html</id><updated>2022-01-12T06:36:03Z</updated><content type="html">We are glad to announce that the Kogito 1.15.0 release is now available! This goes hand in hand with, . From a feature point of view, we included a series of new features and bug fixes, including: * Quarkus Dev Service for Data Index, see * Enhanced support for multi instance sub-process. * Support using Quarkus config mecanish, including profiles and YAML files. * Serverless Workflow support for Workflow Compensation. * Async execution support for process nodes BREAKING CHANGES * Data Index PostgreSQL schema changes, please review release notes for more information. For more details head to the complete. All artifacts are available now: * Kogito runtime artifacts are available on Maven Central. * Kogito examples can be found. * Kogito images are available on. * Kogito operator is available in the in OpenShift and Kubernetes. * Kogito tooling 0.15.0 artifacts are available at the. A detailed changelog for 1.15.0 can be found in. New to Kogito? Check out our website. Click the "Get Started" button. The post appeared first on .</content><dc:creator>Cristiano Nicolai</dc:creator></entry><entry><title type="html">Intelligent data as a service (iDaaS) - Common architectural elements</title><link rel="alternate" href="http://www.schabell.org/2021/12/idaas-common-architectural-elements.html" /><author><name>Eric D. Schabell</name></author><id>http://www.schabell.org/2021/12/idaas-common-architectural-elements.html</id><updated>2022-01-12T06:00:00Z</updated><content type="html">Part 2 - Common architectural elements In  from this series we introduced an architecture around intelligent data as a service (iDaaS) for the healthcare industry. The process was laid out how we approached the use case and how portfolio solutions are the base for researching a generic architecture.  The only thing left to cover was the order in which you'll be led through the details. This article starts the real journey at the very top, with a generic architecture from which we'll discuss the common architectural elements one by one. ARCHITECTURE REVIEW As mentioned before, the architectural details covered here are based on real solutions using open source technologies. The example scenario presented here is a generic common architecture that was uncovered while researching those solutions. It's our intent to provide guidance and not deep technical details. This section covers the visual representations as presented, but it's expected that they'll be evolving based on future research. There are many ways to represent each element in this architecture, but we've chosen a format that we hope makes it easy to absorb. Feel free to post comments at the bottom of this post, or  with your feedback. Now let's take a look at the details in this architecture and outline the solution. FROM SPECIFIC TO GENERIC Before diving into the common elements, it might be nice to understand that this is not a catch all for every possible solution. It's a collection of identified elements that we've uncovered in multiple customer implementations. These elements presented here are then the generic common architectural elements that we've identified and collected into the generic architecture.  It's our intent to provide an architecture for guidance and not deep technical details. You're smart enough to figure out wiring integration points in your own architectures. You're capable of slotting in the technologies and components you've committed to in the past where applicable.  It's our job here to describe the architectural generic components and outline a few specific cases with visual diagrams so that you're able to make the right decisions from the start of your own projects. Another challenge has been how to visually represent the architecture. There are many ways to represent each element, but we've chosen some icons, text and colours that we hope are going to make it all easy to absorb. Now let's take a quick tour of the generic architecture and outline the common elements uncovered in our research. EDGE SERVICES Starting on the left side of the diagram, which is by no means a geographical necessity, there are two elements that represent external edge services that are integrated with the core elements of this architecture.  The first are edge devices, covering basically everything that is used in the field from clinical personnel, patients, to partnering vendors. These can be anything from sensor devices to mobile units such as phones or tablets, but certainly not limited to just these. It's a grouping to identify functionality on the edge of this use case. The second is a catch all for data collection and consumption called external data sources, a broad element containing all of the remote devices. For example, a heart monitoring machine capturing diagnostic information from a heart patient at home. IDAAS CORE These elements in the core to the iDaaS solution and are deployed on a container platform to indicate the cloud-ready nature of this architecture.   Starting on the top right and working down from left to right, the first element is iDaaS knowledge insight. This represents the management applications that provide analytics and insights into the data available across the live platform. This can be setup to provide near-realtime gathering and reporting as organisational need require. This architecture relies on event streaming to react and respond to events within the iDaaS solution, making the iDaaS connect events element central to the solution. This is the element designed for collecting data streams and messages, making it all available to applications and services as subscribers, and pushing notifications as needed to interested parties when certain events happen. Core to any data architecture has to be integration. The iDaaS connect element represents the collection of both integration services and data integration services that are essential to bringing data, messages, and systems throughout a healthcare organisation together in a seamless fashion. The iDaaS connect data distribution element is a solution where routing of data to the desired destination happens. This can be incoming data being routed to the correct data store, or out bound data being routed to the right service, application, or end user. One of the most essential requirements for any healthcare organisation is to maintain compliancy to national laws, data privacy, and other transitional requirements. The iDaaS knowledge conformance element is a set of applications and tools that allow for any organisation to automate compliancy and regulation adherence using rule systems customised to their own local needs. This is a very flexible element that can be designed to ensure that compliancy audit requirements are constantly met. While data distribution can be a simple way to ensure data ends up in the right location, often there is a need for a more intelligent near real-time adjustment to the route data needs to take. Using the iDaaS intelligent data router allows for rule based intelligent routing based on message content, data origin, data destinations, or any other metric required to determine routing. For any healthcare organisation one can expect that there are some complexer processes that might require partially automated processing or even human interaction such as the . An element shown here as the iDaaS event builder is meant for capturing all the process automation needs for any healthcare organisation. It's easy to integrate with data, events, services, applications, and provides a treasure trove of processing metrics to help tune your organisations activities for patients and clinical staff. Finally, there is a need to provide access to internal services through generic application programming interfaces, or API's. The API management element provides all the needed functionality to help expose and connect with services, applications, and more. EXTERNAL SERVICES External services host an array of potential tools, systems, or third-party applications that are used in healthcare organisations. Every organisation has reporting services, either internal or external. These can be Software as a Service (SaaS) tools or just hosted tooling that is external to the organisation. There are any number of big data solutions and tools that can be found in healthcare architectures, often external to the organisation as they are working on very, very large data sets.  Monitoring &amp;amp; logging tools are in abundance together with analytics these also can be hosted externally applications, tooling, or interfaces to be integrated into a healthcare architecture. Many organisations are engaged with other partner data services and need to be able to interact in a timely fashion when sharing or pulling from these data sources. All of the above external services have a need to provide consistent access, which is done using an external API management element shown here that can be a SaaS offering or just hosted externally to the organisation. Finally, both security and any DevOps infrastructure or tooling can be hosted externally and needs to be integrated into the delivery processes for healthcare organisations that make use of them. WHAT'S NEXT This was just a short overview of the common generic elements that make up our architecture for the intelligent data as a service architecture.  An overview of this series on intelligent data as a service architecture: 1. 2. Common architectural elements 3. Example iDaaS architecture 4. Example HL7 and FHIR integration architecture 5. Example iDaaS knowledge and insight architecture Catch up on any articles you missed by following one of the links above. Next in this series, taking a look at an example iDaaS architecture for the intelligent data as a service solution.</content><dc:creator>Eric D. Schabell</dc:creator></entry><entry><title>5 design principles for microservices</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/01/11/5-design-principles-microservices" /><author><name>Bob Reselman</name></author><id>fa3871c0-2c6a-48cd-a26e-729743bc6c7b</id><updated>2022-01-11T07:00:00Z</updated><published>2022-01-11T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/topics/microservices"&gt;Microservices&lt;/a&gt; are becoming increasingly popular to address shortcomings in monolithic applications. This article is the first in a three-part series that describes the design principles for a microservices-oriented application (MOA), shows how companies tend to evolve to use microservices, and describes the trade-offs in using microservices.&lt;/p&gt; &lt;h2&gt;What is a monolithic application?&lt;/h2&gt; &lt;p&gt;The term &lt;em&gt;monolithic&lt;/em&gt; applies to tightly integrated applications where it is hard to change one function without also recoding other parts of the application. Components in a monolithic application might be distributed among many machines, but they remain highly dependent on one another. Not only does the addition of a new feature have ripple effects throughout the code, but deploying the change requires retesting and redeploying the entire application. These upgrades can be labor-intensive and hazardous, particularly when an application has hundreds of thousands or even millions of users.&lt;/p&gt; &lt;p&gt;When IT departments had the luxury of releasing software every six months, this type of upgrade process was tolerable. But modern business demands force releases to happen weekly, daily, or even more often, so the labor and risk inherent in upgrading monolithic applications become untenable.&lt;/p&gt; &lt;p&gt;Something has to change. That change is the transformation to the microservices-oriented application (MOA).&lt;/p&gt; &lt;h2&gt;What is a microservices-oriented application?&lt;/h2&gt; &lt;p&gt;An MOA breaks its logic into small, well-encapsulated services that are distributed over several computing devices in a loosely coupled manner. Each service lives at a distinct IP address on the network and exposes a public interface that is language-agnostic. The most popular type of language-agnostic interface is a &lt;a href="https://www.redhat.com/en/topics/api/what-is-a-rest-api"&gt;REST API&lt;/a&gt;, but other models for communication exist. Microservices also generally get deployed as &lt;a href="https://developers.redhat.com/topics/containers"&gt;containers&lt;/a&gt; when it's time to go live.&lt;/p&gt; &lt;p&gt;Typically, some mechanism behind the scenes coordinates the microservices to create a unified application experience. Because each microservice is well-encapsulated, its code can be updated quickly with minimal side effects. This makes maintenance easier and scaling faster.&lt;/p&gt; &lt;p&gt;The benefits of an MOA can be significant, but they come with a price. You need to know a thing or two about microservice design to implement an MOA effectively—you can't make it up as you go along.&lt;/p&gt; &lt;p&gt;The purpose of this series is to describe the principles involved in choosing the microservices architecture, along with the pros and cons. This first article presents the five basic principles of microservices-oriented application design. The next part of the series explains the evolution of modern applications and why they lead to an MOA, and the third part finishes the series with trade-offs that microservices make.&lt;/p&gt; &lt;h2&gt;Five design principles for microservices&lt;/h2&gt; &lt;p&gt;The five basic principles of microservice application design are:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;A microservice has a single concern.&lt;/li&gt; &lt;li&gt;A microservice is discrete.&lt;/li&gt; &lt;li&gt;A microservice is transportable.&lt;/li&gt; &lt;li&gt;A microservice carries its own data.&lt;/li&gt; &lt;li&gt;A microservice is ephemeral.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Let’s look at the details.&lt;/p&gt; &lt;h3&gt;1. A microservice has a single concern&lt;/h3&gt; &lt;p&gt;Having a single concern means that a microservice should do one thing and one thing only. For example, if the microservice is intended to support authentication, it should do authentication only. This means that its interface should expose only access points that are relevant to authentication. And internally, the microservice should have authentication behavior only. For example, there should be no side behavior such as providing employee contact information in the authentication response.&lt;/p&gt; &lt;p&gt;Having a single concern makes the microservice easier to maintain and scale. Having a single concern also goes hand-in-hand with the next principle.&lt;/p&gt; &lt;h3&gt;2. A microservice is discrete&lt;/h3&gt; &lt;p&gt;A microservice must have clear boundaries separating it from its environment. Another way to think about this principle is that a microservice must be well-encapsulated. This means that all logic and data relevant to a microservice's single concern must be encapsulated into a single deployment unit. Examples of units for discrete deployment are a &lt;a href="https://developers.redhat.com/topics/linux/"&gt;Linux&lt;/a&gt; container, a WebAssembly binary, a &lt;a href="https://developers.redhat.com/topics/dotnet/"&gt;.NET&lt;/a&gt; DLL, a &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js&lt;/a&gt; package, and a &lt;a href="https://developers.redhat.com/topics/enterprise-java/"&gt;Java&lt;/a&gt; JAR file, to name a few.&lt;/p&gt; &lt;p&gt;Also, a discrete microservice is hosted in a distinct source control repository and is subject to its own &lt;a href="https://developers.redhat.com/topics/ci-cd/"&gt;CI/CD&lt;/a&gt; (continuous integration/continuous delivery) process. The microservice becomes part of a larger application after deployment. But from development through testing and on to release, each microservice is isolated from all other microservices. When a microservice is discrete, it becomes easily transportable, which is the next principle we'll cover.&lt;/p&gt; &lt;h3&gt;3. A microservice is transportable&lt;/h3&gt; &lt;p&gt;A transportable microservice can be moved from one runtime environment to another with little effort. Perhaps currently, the optimal form of a transportable microservice is a Linux container image.&lt;/p&gt; &lt;p&gt;Usually, a Linux container image is hosted in an image repository such as &lt;a href="https://quay.io"&gt;Red Hat Quay.io&lt;/a&gt;. The container image can be targeted to any destination from that image repository, so a variety of applications can use the image. This is all possible because the microservice is encapsulated into a discrete deployment unit that can be transported to any destination. The encapsulation removes from developers all tasks except configuration and deployment.&lt;/p&gt; &lt;p&gt;Transportable microservices also make them easier to use in an automated or declarative deployment process.&lt;/p&gt; &lt;h3&gt;4. A microservice carries its own data&lt;/h3&gt; &lt;p&gt;A microservice should have its own data storage mechanism that is isolated from all other microservices. The only way data can be shared with other microservice is by way of a public interface that the microservice publishes.&lt;/p&gt; &lt;p&gt;This principle imposes some discipline on data sharing: For instance, the particular data schema used by each microservice has to be well-documented. The design rules out behind-the-scenes hanky-panky that makes data hard to access or understand.&lt;/p&gt; &lt;p&gt;The principle that a microservice carries its own data is hard for many developers to accept. The common argument against a microservice carrying its own data is that the principle leads to a proliferation of data redundancy.&lt;/p&gt; &lt;p&gt;For example, imagine an e-commerce application. That application might have a microservice that manages customer profile information. The application has another microservice that handles purchases. When the principle that every microservice carries its own data is in force, it's quite possible that the purchases microservice might have data that is redundant with the customer profile microservice. Such data redundancy goes against the grain of developers who embrace the DRY principle (Don't Repeat Yourself).&lt;/p&gt; &lt;p&gt;On the other hand, developers who embrace the carry-their-own data principle understand the benefits and have adjusted accordingly. When a microservice carries its own data, any strange behavior is confined within the microservice.&lt;/p&gt; &lt;p&gt;When microservices try to share data, one microservice can make a change that causes a side effect in another microservice. This is fundamentally a bad way of doing business.&lt;/p&gt; &lt;p&gt;One of the key benefits of a microservice carrying its own data is that it enforces all of the other principles. This is particularly important when it comes to the final principle: That a microservice is ephemeral.&lt;/p&gt; &lt;h3&gt;5. A microservice is ephemeral&lt;/h3&gt; &lt;p&gt;The principle that a microservice is ephemeral means that it can be created, destroyed, and replenished on demand on a given target easily, quickly, and with no side effects. The standard operating expectation is that microservices come and go all the time, sometimes due to system failure and sometimes due to scaling demands.&lt;/p&gt; &lt;p&gt;This scenario is common in a &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; environment that uses the &lt;a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/"&gt;Horizontal Pod Autoscaler&lt;/a&gt; (HPA) to accommodate scaling demands. The HPA creates and destroys containers according to momentary demands. Each time a container is created, an IP address is assigned dynamically. There are even situations where port numbers will be assigned dynamically too. Such are the impacts of ephemeral computing.&lt;/p&gt; &lt;p&gt;As a result, this principle that a microservice is ephemeral has two basic implications. The first is that developers need to create microservices that are good citizens in the application domain. This means implementing graceful startup and shutdown behavior within the microservice.&lt;/p&gt; &lt;p&gt;The second implication is that when programming their microservices, developers rely on runtime configuration settings to define external dependencies. This hand-off to an external configuration differs greatly from creating a monolithic application, where most dependencies are known at design time and are baked into the code. In microservice development, they're not. Instead, developers rely upon dynamic configuration to establish dependencies that are both internal and external to the microservice.&lt;/p&gt; &lt;p&gt;As strange as it might sound, coding to an ephemeral environment requires developers to accept that there are "known unknowns" that will be apparent at runtime. Hence, the need to program accordingly.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;The next article in this series will show how some companies incrementally move from a monolithic application to microservices. In the meantime, you can find more resources on our &lt;a href="https://developers.redhat.com/topics/microservices"&gt;microservices topic page&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/01/11/5-design-principles-microservices" title="5 design principles for microservices"&gt;5 design principles for microservices&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Bob Reselman</dc:creator><dc:date>2022-01-11T07:00:00Z</dc:date></entry><entry><title type="html">Announcing DashBuilder 0.14.1</title><link rel="alternate" href="https://blog.kie.org/2022/01/announcing-dashbuilder-0-14-1.html" /><author><name>William Siqueira</name></author><id>https://blog.kie.org/2022/01/announcing-dashbuilder-0-14-1.html</id><updated>2022-01-10T21:04:11Z</updated><content type="html">We are glad to announce that we are releasing DashBuilder 0.14.1! The major change for this new release is the adoption of Quarkus as the backend for Dashbuilder Runtime and the introduction of Dashbuilder Authoring, a new tool to create dashboards. The project code has also moved to Kogito tooling, which means that it will be regularly released with other kogito tooling tools. You may be asking why 0.14.1 and not 8.0. The reason is that Dashbuilder is now part of  and will follow its release cycle. DASHBUILDER IMPROVEMENTS AND NEW FEATURES The major change for 0.14.1 is that now Dashbuilder Runtime is based on Quarkus. It means quicker startup time (10x faster to start) and better overall performance. As a consequence we now have some changes: * Dev mode now uses SSE (Server-Sent Event); * Client to server communication is done via REST; * HTTP Compression comes from Quarkus; * New uploaded can be disabled using system property dashbuilder.runtime.allowUpload; When false new uploads will not be supported; * SQL DataSource configuration can be done using system properties: java -Ddashbuilder.datasources=sample \ -Ddashbuilder.datasource.sample.jdbcUrl=jdbc:mariadb://localhost:3306/sample \ -Ddashbuilder.datasource.sample.providerClassName=org.mariadb.jdbc.Driver \ -Ddashbuilder.datasource.sample.maxSize=10 \ -Ddashbuilder.datasource.sample.principal=repasse \ -Ddashbuilder.datasource.sample.credential=repasse \ -jar target/dashbuilder-runtime-8.0.0-Alpha.jar Make sure to change JDBC URL and Driver according to the used DB. The supported databases are the same as supported by . * Bean datasets now work by simply adding the JAR with the bean dataset and all its dependencies to the classpath; All features and dashboards that used to work on 7.x should work on 8.x with the exception of ElasticSearch, which was removed in this new version. To author dashboards we are releasing Dashbuilder Authoring, which we will talk more about later in this post. RUNNING DASHBUILDER RUNTIME 0.14.1 Download the distribution JAR and run it using Java 11 or later with the java -jar command. java -jar dashbuilder-runtime-app.jar The bootstrap switches for the 7.x continuous working with this new version. For example, this is how you run with a static dashboard: java -Ddashbuilder.runtime.import=/path/to/dashboard.zip -jar dashbuilder-runtime-app.jar Or in multi mode: java -Ddashbuilder.runtime.multi=true -jar dashbuilder-runtime-app.jar DASHBUILDER AUTHORING We are now releasing DashBuilder Authoring, a tool to author dashboards! Datasets, pages and navigation is saved directly in the filesystem. The path to the saved artifacts can be configured using system property org.dashbuilder.project.location (default is ./dashbuilder) and you can also automatically export the dashboard by setting a path to the exported ZIP using the system property dashbuilder.export.location. DashBuilder Authoring is distributed in the form of bootable JAR or a WAR that can be deployed on Wildfly 23.0.2. Deploy the WAR must be used when use of SQL dataset is required. The bootable JAR can be executed using Java 11+: java -jar dashbuilder-authoring-bootable.jar CONTAINER IMAGES It is possible to download and run dashbuilder binaries locally or you can try DashBuilder now using the following container images: Runtime quay.io/kogito_tooling_bot/dashbuilder-runtime You can pass Java properties using the JAVA_OPTS environment variable. For example, to run Dashbuilder Runtime in multi mode use the following command: podman run -p 8080:8080 -e “JAVA_OPTS=-Ddashbuilder.runtime.multi=true -Ddashbuilder.runtime.allowUpload=true” -dti quay.io/kogito_tooling_bot/dashbuilder-runtime:0.14.1 Authoring quay.io/kogito_tooling_bot/dashbuilder-authoring Dashbuilder Authoring uses the WAR bits and uses as base Wildfly 23.0.2 image. EXTERNAL DATASETS This release also includes a new dataset provider type called “External DataSets”. External Datasets are based in JSON, basically any JSON regular 2×2 array can be a dataset now. More information can be found in , but this will be covered later in its own post. CONCLUSION We introduced the new DashBuilder! In the next post we will share a new repository of sample dashboards that you can run with DashBuilder Runtime, so stay tuned! The post appeared first on .</content><dc:creator>William Siqueira</dc:creator></entry><entry><title type="html">Business Automation with an External AMQ Broker using Operators</title><link rel="alternate" href="https://blog.kie.org/2022/01/business-automation-with-an-external-amq-broker-using-operators.html" /><author><name>Trevor Royer</name></author><id>https://blog.kie.org/2022/01/business-automation-with-an-external-amq-broker-using-operators.html</id><updated>2022-01-10T14:43:22Z</updated><content type="html">In this article we will setup a new AMQ Broker using the Red Hat Integration - AMQ Broker Operator, a Kie Server instance using the Business Automation Operator, and configure that Business Automation instance to utilize the AMQ Broker for all of it’s messaging. Moving the AMQ Broker outside of our Kie Server instance can give us several different advantages. By moving the internal message broker out of the Kie Server and into an external instance we are able to reduce the workload and memory footprint in our Kie Server containers. Additionally, with an external message broker we are able to leverage new micro-service design patterns that can interact with external services. Finally, when attempting to scale our Kie Server instance beyond a single pod, the external AMQ Broker becomes essential to providing fail-over capabilities between Kie Server pods, enabling the ability utilize auto-scaling, and allowing us to distribute asynchronous process instance tasks across multiple pods. To learn more about how to build a BPMN Process that utilizes the AMQ Broker as part of a micro-service interacting with an external web service, take a look at the blog . INSTALL THE AMQ BROKER OPERATOR We will start by installing the AMQ Broker Operator in our project. To begin make sure you have selected the Project you wish to install the applications in, or create a new Project. From the Administrator page on the OpenShift Web Console select Operators &gt; OperatorHub from the panel on the left hand side and search for AMQ Broker in the search box. Select the option for Red Hat Integration - AMQ Broker for RHEL 8 (Multiarch). Click on the Install box to bring up the install options page. Set the Channel to 7.x and update the Install Mode to A specific namespace on the cluster. Double check that the correct namespace is selected. The 7.x version of the operator supports both cluster installs and namespaced installs of the operator. Cluster installs enable users to install the AMQ components in any namespace with a single version of the Operator installed in the entire cluster. Namespaced installs only allow the operator to monitor for resources created in the namespace the operator was installed. Generally, cluster installs require Cluster Admin permissions and will expose the operator to all users of the cluster. Click Install to start the installation. To verify the install succeeded navigate to Operators &gt; Installed Operators from the left hand navigation panel. You should see the Red Hat Integration - AMQ Broker for RHEL 8 (Multiarch) listed with a Succeeded Status. INSTALL THE BUSINESS AUTOMATION OPERATOR Next we will install the Business Automation Operator. Just like before, navigate to Operators &gt; OperatorHub and search for Business Automation in the search box. Select the Business Automation Operator and click Install. For Business Automation the Stable channel should already be selected. Double check that the correct namespace is selected and click Install. Just like before you can double check that the install succeeded under Operators &gt; Installed Operators and waiting for the Status to report as Succeeded. SETUP THE AMQ BROKER Next we will utilize the AMQ Broker Operator to setup an AMQ Broker instance and the addresses Business Automation requires. To create an AMQ Broker instance navigate to the AMQ Broker Operator page by going to Operators &gt; Installed Operators and selecting Red Hat Integration - AMQ Broker for RHEL 8 (Multiarch). On the AMQ Broker operator page click Create instance for the AMQ Broker object. This will bring us to a page with a Form view already started for us. We can also switch to a YAML view where we can edit the YAML directly. We will start by updating the name and application fields to be amq-demo and amq-demo-app. Then we will add the a section for console.expose: true under the spec to enable a web console for the AMQ Broker. Click Create at the bottom of the page to create the new AMQ Broker instance. The AMQ Operator will now utilize our resource definition to create a new AMQ Broker instace. To validate the setup, navigate to Workloads &gt; Pods on the left hand panel. You should see an instace of the AMQ Broker Operator, and the Business Automation Operator that we previously installed. A new pod called amq-demo-ss-0 should be listed. The pod may take a few minutes to successfully start. After the first pod has become ready, a second pod called amq-demo-ss-1 should start as well. SETUP THE AMQ ADDRESSES Next we will need to create the default queues that our Kie Server application will utilize. Just like the Broker object, the AMQ Broker Operator provides us a default YAML template or a Form we can utilize to create the AMQ Broker Address objects. Alternatively, we can create multiple objects at once using the YAML import feature. Click on the + icon in the top right hand corner to open the Import YAML page and copy and paste the following into the editor: --- kind: ActiveMQArtemisAddress apiVersion: broker.amq.io/v2alpha3 metadata: name: demo-audit namespace: rhpam-amq-demo spec: addressName: DEMO_AUDIT queueName: DEMO_AUDIT routingType: anycast --- kind: ActiveMQArtemisAddress apiVersion: broker.amq.io/v2alpha3 metadata: name: demo-executor namespace: rhpam-amq-demo spec: addressName: DEMO_EXECUTOR queueName: DEMO_EXECUTOR routingType: anycast --- kind: ActiveMQArtemisAddress apiVersion: broker.amq.io/v2alpha3 metadata: name: demo-request namespace: rhpam-amq-demo spec: addressName: DEMO_REQUEST queueName: DEMO_REQUEST routingType: anycast --- kind: ActiveMQArtemisAddress apiVersion: broker.amq.io/v2alpha3 metadata: name: demo-response namespace: rhpam-amq-demo spec: addressName: DEMO_RESPONSE queueName: DEMO_RESPONSE routingType: anycast --- kind: ActiveMQArtemisAddress apiVersion: broker.amq.io/v2alpha3 metadata: name: demo-signal namespace: rhpam-amq-demo spec: addressName: DEMO_SIGNAL queueName: DEMO_SIGNAL routingType: anycast Click Create to create all of the Address objects. To validate the creation, return to the Operators &gt; Installed Operators page and select the AMQ Broker Operator. Navigate to the AMQ Broker Address tab and you should see all of the AMQ Broker Address objects we have created. Next, we will validate that the new addresses have been created inside of the AMQ Broker. Navigate to Networking &gt; Routes in the admin panel. You should see two routes for the AMQ Broker titled something like amq-demo-&lt;random-id&gt;-0-svc-rte and amq-demo-&lt;random-id&gt;-1-svc-rte. Click on the URL associated with either one. In the new window click on the link for Management Console. An OpenShift OAuth login page will appear. Use the same username and password you use to login to OpenShift. On the console you should see the addresses we created under amq-broker &gt; addresses. SETUP THE KIE SERVER Navigate to Operators &gt; Installed Operators and select the Business Automation Operator. Select the option Create instance for the KieApp. A new YAML template will appear. Select the radial button to change the view to the Form view. By default the Name field is filled out as well as the Environment. Update the Name to rhpam-demo and select rhpam-trial for the Environment. For more information on the various environment options, review the official documentation for . In the form, open the Objects &gt; Servers section and choose the option to Add Server. Locate the Env option in the newly created Server object and click Add Env. Create a new environment variable with the following name and value: name: MQ_SERVICE_PREFIX_MAPPING value: DEMO_amq7=AMQ Next, click the option to add another Env and create a new variable names AMQ_USERNAME. Leave Value empty and instead click the options for Value From &gt; Secret Key Ref. In the Key field enter AMQ_USER for the name enter amq-demo-credentials-secret. The amq-demo-credentials-secret is an object created by the AMQ Broker Operator when it created our AMQ Broker instance. AMQ_USER is the name of secret we would like to reference inside of amq-dem-credentials-secret. To view the contents of the amq-demo-credentials-secret you can find it under Workloads &gt; Secrets in the left hand panel. Add another env variable using the Secret Key Ref named AMQ_PASSWORD. For the Key field enter AMQ_PASSWORD and the same secret as before, amq-demo-credentials-secret, for the name. The next env variable with will create should be called DEMO_AMQ_TCP_SERVICE_HOST and set the value equal to amq-demo-hdls-svc. This address is a Kubernetes Service object created by the AMQ Broker Operator for our AMQ Broker instance to assist with internal networking between pod. To validate the name of your service, you can view it under Networking &gt; Services in the left hand panel. Continue this process to create all of the following environment variables: - name: DEMO_AMQ_TCP_SERVICE_PORT value: '61616' - name: AMQ_PROTOCOL value: tcp - name: KIE_SERVER_JMS_QUEUE_EXECUTOR value: DEMO_EXECUTOR - name: KIE_SERVER_JMS_QUEUE_RESPONSE value: DEMO_RESPONSE - name: KIE_SERVER_JMS_QUEUE_REQUEST value: DEMO_REQUEST - name: KIE_SERVER_JMS_QUEUE_SIGNAL value: DEMO_SIGNAL - name: KIE_SERVER_JMS_QUEUE_AUDIT value: DEMO_AUDIT - name: AMQ_QUEUES value: DEMO_EXECUTOR, DEMO_RESPONSE, DEMO_REQUEST, DEMO_SIGNAL, DEMO_AUDIT - name: EJB_RESOURCE_ADAPTER_NAME value: activemq-ra-remote Once you have finished creating all of the variables you should be able to switch back to the YAML view at the top to review your final configuration. It should look the same as you see below: apiVersion: app.kiegroup.org/v2 kind: KieApp metadata: name: rhpam-demo spec: objects: servers: - env: - name: MQ_SERVICE_PREFIX_MAPPING value: DEMO_amq7=AMQ - name: AMQ_USERNAME valueFrom: secretKeyRef: name: amq-demo-credentials-secret key: AMQ_USER - name: AMQ_PASSWORD valueFrom: secretKeyRef: name: amq-demo-credentials-secret key: AMQ_PASSWORD - name: AMQ_PROTOCOL value: tcp - name: DEMO_AMQ_TCP_SERVICE_HOST value: amq-demo-hdls-svc - name: DEMO_AMQ_TCP_SERVICE_PORT value: '61616' - name: KIE_SERVER_JMS_QUEUE_EXECUTOR value: DEMO_EXECUTOR - name: KIE_SERVER_JMS_QUEUE_RESPONSE value: DEMO_RESPONSE - name: KIE_SERVER_JMS_QUEUE_REQUEST value: DEMO_REQUEST - name: KIE_SERVER_JMS_QUEUE_SIGNAL value: DEMO_SIGNAL - name: KIE_SERVER_JMS_QUEUE_AUDIT value: DEMO_AUDIT - name: AMQ_QUEUES value: &gt;- DEMO_EXECUTOR, DEMO_RESPONSE, DEMO_REQUEST, DEMO_SIGNAL, DEMO_AUDIT - name: EJB_RESOURCE_ADAPTER_NAME value: activemq-ra-remote environment: rhpam-trial Click Create to create the new KieApp. The Business Automation Operator will deploy several new resources based on the KieApp definition. To view all of the resources the operator creates, you can select the KieApp instance from the KieApp tab of the Business Automation Installed Operators page. Then click on the Resources tab to view all of the resources deployed by the KieApp. You can also monitor the deployment of the KieApp pods by navigating to Workloads &gt; Pods to view the deployed pods. To access the new Business Central instance, navigate to Networking &gt; Routes and click on the URL for the rhpam-demo-rhdmcentr route. The default username should be adminUser and the default password is RedHat. To validate that our new RHPAM instance is utilizing our AMQ cluster, jump back over into the AMQ Broker console and click on the Consumers tab. You should now see our addresses listed under Consumers. The IP address under Remote Address should match the IP address for our Kie Server pod. GETTING WITH GITOPS Another advantage we haven’t discussed of relying on Operators in OpenShift to deploy the components of our application is that we will be GitOps ready! For those of you new to GitOps, GitOps is a DevOps approach to storing all of the components of your application in a declarative format (like the YAML files created by OpenShift/Kubernetes) as part of a git repo. Then you utilize a tool such as OpenShift GitOps (ArgoCD) to continuously deploy those implementations to your cluster. Any changes to those resources in your git repo are automatically synced by the GitOps tool to your cluster. By utilizing operators to deploy our AMQ Broker and Business Automation components we can simply take the Custom Resources we create as part of our implementation, and move those resources into a git repo as part of our implementation. GitOps is a powerful strategy when implementing solutions, especially when working in higher level environments such as a Production deployment. Instead of creating a complex procedure for deploying an application, we can simply capture the YAML objects we create for the deployment in a git repo and use that as our basis for deploying in any future environment. This helps to save time, ensure consistency across environments, and avoid configuration drift between environments. For the items deployed in this post we would need to capture YAML objects for the following: * AMQ Broker Operator Subscription * Business Automation Operator Subscription * AMQ Broker Instance * AMQ Broker Addresses * KieApp Instance CONCLUSION Now that our AMQ Broker and Business Automation implementations are up and running, we are ready to start developing a process and external service that communicate using our AMQ Broker. Be sure to head over to to learn more on developing those services. The post appeared first on .</content><dc:creator>Trevor Royer</dc:creator></entry><entry><title>The GDB developer’s GNU Debugger tutorial, Part 2: All about debuginfo</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/01/10/gdb-developers-gnu-debugger-tutorial-part-2-all-about-debuginfo" /><author><name>Keith Seitz</name></author><id>3bd1b585-5cde-4f48-9529-8dd34f297e51</id><updated>2022-01-10T07:00:00Z</updated><published>2022-01-10T07:00:00Z</published><summary type="html">&lt;p&gt;In the first article of this series, &lt;a href="https://developers.redhat.com/blog/2021/04/30/the-gdb-developers-gnu-debugger-tutorial-part-1-getting-started-with-the-debugger"&gt;Getting started with the debugger&lt;/a&gt;, I introduced the &lt;a href="https://www.sourceware.org/gdb/"&gt;GNU Debugger&lt;/a&gt; (GDB) and walked you through its common startup options and processes. As I promised in Part 1, this article introduces the debugging information that is used to describe compiled code.&lt;/p&gt; &lt;h2&gt;What is debugging information?&lt;/h2&gt; &lt;p&gt;Put simply, debugging information (&lt;em&gt;debuginfo&lt;/em&gt; for short) is output by &lt;a href="https://developers.redhat.com/products/gcc-clang-llvm-go-rust/overview"&gt;compilers&lt;/a&gt; and other tools to tell other development tools about compiled code. Debuginfo communicates important information about a program or library, including line tables that map executable code back to the original source code; file names and included files; descriptions of functions, variables, and types in the program; and much more.&lt;/p&gt; &lt;p&gt;All of this information is used by other tools, including GDB, to let users step through a program's source code line by line, inspect variables and stack backtraces, and perform other essential development and debugging tasks.&lt;/p&gt; &lt;p&gt;&lt;a href="https://dwarfstd.org"&gt;DWARF&lt;/a&gt; is the debuginfo standard used on the majority of operating systems today, including GNU &lt;a href="https://developers.redhat.com/topics/linux"&gt;Linux&lt;/a&gt;, BSD, and other Unix derivatives. It is language- and architecture-agnostic. The same debuginfo format is valid for any architecture (including x86_64, PowerPC, or RISC) or any language (including &lt;a href="https://developers.redhat.com/topics/c"&gt;C and C++&lt;/a&gt;, Ada, D, or Rust).&lt;/p&gt; &lt;p&gt;The debugging information entries (DIEs) output by compilers and other tools provide a low-level description of the compiled source code. Every DIE consists of an identifying tag and various attributes that relay important information about the entity being represented. Various source language concepts, such as functions, may be described by several DIEs arranged in an essentially hierarchical order, with child DIEs refining the description of their parent DIE.&lt;/p&gt; &lt;h2&gt;Reading debuginfo: A simple use case&lt;/h2&gt; &lt;p&gt;To illustrate a (very) simple case, consider the following “Hello, World!” example in C:&lt;/p&gt; &lt;pre&gt; 1 #include &lt;stdio.h&gt; 2 3 int 4 main (void) 5 { 6 int counter; 7 8 for (counter = 0; counter &lt; 10; ++counter) 9 printf ("Hello, World (%d)!\n", counter); 10 11 return 0; 12 } &lt;/pre&gt; &lt;p&gt;If this were saved into a file named &lt;code&gt;hello.c&lt;/code&gt; and compiled with debugging information (see the discussion on &lt;a href="https://developers.redhat.com/blog/2021/04/30/the-gdb-developers-gnu-debugger-tutorial-part-1-getting-started-with-the-debugger#compiler_options"&gt;compiler flags&lt;/a&gt; in the first article in this series), the output to describe this program might look like the pseudo-DIEs here:&lt;/p&gt; &lt;pre&gt; DW_TAG_compile_unit DW_AT_producer: “gcc -g” DW_AT_language: “C” DW_AT_name: “hello.c” DW_AT_comp_dir: “/home/keiths” DW_TAG_base_type DW_AT_name: “int” DW_TAG_subprogram DW_AT_name: “main” DW_AT_decl_file: 1 DW_AT_decl_line: 4 DW_AT_type: &lt;em&gt;link to DIE describing “int”&lt;/em&gt; DW_TAG_variable DW_AT_name: “counter” DW_AT_decl_file: 1 DW_AT_decl_line: 6 DW_AT_type: &lt;em&gt;link to DIE describing “int”&lt;/em&gt; &lt;em&gt;much more&lt;/em&gt; &lt;/pre&gt; &lt;p&gt;The resulting debugging information describes a compilation unit with the name &lt;code&gt;hello.c&lt;/code&gt;, containing a function named &lt;code&gt;main&lt;/code&gt; that is defined on line 4 of &lt;code&gt;hello.c&lt;/code&gt;. This function contains a variable named &lt;code&gt;counter&lt;/code&gt; that is defined on line 6, and so on. Real DWARF output would, of course, contain much more information, such as where these entities exist in memory, but this simple example demonstrates the hierarchical nature of DWARF DIEs.&lt;/p&gt; &lt;h2&gt;Where is debuginfo stored and how can you get it?&lt;/h2&gt; &lt;p&gt;Debugging information is stored in several places, including local and remote file systems. Depending on what you are debugging, GDB and other tools search the following locations for the desired debugging information.&lt;/p&gt; &lt;h3&gt;1. The program's ELF sections&lt;/h3&gt; &lt;p&gt;For programs that you write yourself, such as my “Hello, World!” example, the debugging information is stored in the ELF file itself. When the file is loaded into tools that use debuginfo, the tools will read the various ELF sections related to debugging information (&lt;code&gt;.debug_abbrev&lt;/code&gt;, &lt;code&gt;.debug_aranges&lt;/code&gt;, &lt;code&gt;.debug_info&lt;/code&gt;, etc.) as needed.&lt;/p&gt; &lt;h3&gt;2. Separate debug directories by name&lt;/h3&gt; &lt;p&gt;For programs and libraries distributed by vendors such as Red Hat, all debugging information is stripped from programs and libraries and saved into separate packages. For example, the program &lt;code&gt;cp&lt;/code&gt; is in the &lt;code&gt;coreutils&lt;/code&gt; package, and its debugging information is in the &lt;code&gt;coreutils-debuginfo&lt;/code&gt; package.&lt;/p&gt; &lt;p&gt;To install the debuginfo for any distribution-supplied package such as &lt;code&gt;coreutils&lt;/code&gt;, use &lt;code&gt;dnf debuginfo-install coreutils&lt;/code&gt;. This will also download the associated &lt;code&gt;debugsrc&lt;/code&gt; package containing the package’s source code.&lt;/p&gt; &lt;p&gt;These separate debuginfo packages are installed under &lt;code&gt;/usr/lib/debug&lt;/code&gt;, a special directory that holds distribution-wide debuginfo. Tools such as GDB know to look in this directory when searching for any missing debugging information.&lt;/p&gt; &lt;h3&gt;3. Separate debug directories by build ID&lt;/h3&gt; &lt;p&gt;Build IDs are ELF note segments in the object file. A &lt;em&gt;build ID&lt;/em&gt; is essentially a hash that uniquely identifies any given version of a program or library. Tools will look for this special note segment to find a build ID and use it to locate the debuginfo in the build ID debug directory, &lt;code&gt;/usr/lib/debug/.build-id&lt;/code&gt;. (Later in the article, you will learn how to query an object file for its build ID.)&lt;/p&gt; &lt;h3&gt;4. debuginfod server by build ID&lt;/h3&gt; &lt;p&gt;Many tools, including GDB, support the use of a debuginfod server, which allows users to download debugging information on demand from centralized servers. Any debuginfo downloaded by debuginfod will be stored in a cache directory in the user’s home directory, &lt;code&gt;$HOME/.cache/debuginfod_client&lt;/code&gt;.&lt;/p&gt; &lt;h2&gt;Using debuginfod&lt;/h2&gt; &lt;p&gt;To use debuginfod, set the environment variable &lt;code&gt;DEBUGINFOD_URLS&lt;/code&gt; to point at any servers you want to check for debuginfo. The upstream federated server maintained by the &lt;a href="https://sourceware.org/elfutils/"&gt;elfutils project&lt;/a&gt; can be used to automatically access debugging information for any maintained distribution:&lt;/p&gt; &lt;pre&gt; $ export DEBUGINFOD_URLS="https://debuginfod.elfutils.org/ $DEBUGINFOD_URLS" &lt;/pre&gt; &lt;p&gt;GDB 12 also contains new commands to control the debuginfod client library (each command also has a corresponding show equivalent):&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;set debuginfod enabled on/off/ask&lt;/code&gt;: Enables or disables GDB's debuginfod support. If set to &lt;code&gt;ask&lt;/code&gt; (the default when debuginfod support is available), GDB will ask the user to confirm the use of debuginfod to download missing debugging information for the current session. To permanently enable this, add &lt;code&gt;set debuginfod enabled on&lt;/code&gt; to your &lt;code&gt;.gdbinit&lt;/code&gt; start-up script. (For more information on &lt;a href="https://developers.redhat.com/blog/2021/04/30/the-gdb-developers-gnu-debugger-tutorial-part-1-getting-started-with-the-debugger#startup_scripts"&gt;start-up scripts&lt;/a&gt;, see the previous article in this series.)&lt;/li&gt; &lt;li&gt;&lt;code&gt;set debuginfod urls &lt;em&gt;LIST&lt;/em&gt;&lt;/code&gt;: Allows you to provide a space-separated list of URLs from which to query for debuginfo. If debuginfod support is enabled in GDB, this defaults to the &lt;code&gt;DEBUGINFOD_URLS&lt;/code&gt; environment variable.&lt;/li&gt; &lt;li&gt;&lt;code&gt;set debuginfod verbose on/off&lt;/code&gt;: Controls the verbosity of debuginfod client library messages. To suppress these messages, set this value to &lt;code&gt;off&lt;/code&gt;.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;You can also set up your own debuginfod server. See &lt;a href="https://developers.redhat.com/blog/2019/12/17/deploying-debuginfod-servers-for-your-developers"&gt;Frank Eigler’s excellent article&lt;/a&gt; about that for more information.&lt;/p&gt; &lt;h2&gt;When is debuginfo loaded?&lt;/h2&gt; &lt;p&gt;GDB loads debugging information lazily unless the &lt;code&gt;--readnow&lt;/code&gt; option is passed on the command line. Whenever a new object file is encountered (that is, when a program is loaded into GDB or a shared library is loaded at runtime), GDB will quickly scan its debuginfo to collect important information. GDB will typically not read full symbols for any object file until it is requested by the user. For example, attempting to set a breakpoint on a function will cause GDB to expand debugging information for the given function’s compilation unit.&lt;/p&gt; &lt;h2&gt;How to inspect debuginfo&lt;/h2&gt; &lt;p&gt;There are a number of useful command-line tools for inspecting debugging information. These tools are typically a part of the &lt;code&gt;binutils&lt;/code&gt; and &lt;code&gt;elfutils&lt;/code&gt; packages on Red Hat-based systems.&lt;/p&gt; &lt;p&gt;To inspect build IDs, install the &lt;code&gt;elfutils&lt;/code&gt; package. To inspect debugging information more generally, install both &lt;code&gt;binutils&lt;/code&gt; and &lt;code&gt;elfutils&lt;/code&gt; packages.&lt;/p&gt; &lt;pre&gt; $ sudo dnf install binutils elfutils &lt;/pre&gt; &lt;h3&gt;Inspect the build ID&lt;/h3&gt; &lt;p&gt;You can use the &lt;code&gt;eu-unstrip&lt;/code&gt; program to get an object file’s build ID:&lt;/p&gt; &lt;pre&gt; $ eu-unstrip -n -e /usr/bin/ls 0+0x23540 c1e1977d6c15f173215ce21f017c50aa577bb50d@0x378 /usr/bin/ls /usr/lib/debug/usr/bin/ls-8.32-30.fc34.x86_64.debug &lt;/pre&gt; &lt;p&gt;The build ID is the second element up to the terminating &lt;code&gt;@&lt;/code&gt;: &lt;code&gt;c1e1977d6c15f173215ce21f017c50aa577bb50d&lt;/code&gt;, in this case. If the debuginfo for this program (&lt;code&gt;coreutils-debuginfo&lt;/code&gt;) is installed, it is easy to verify it:&lt;/p&gt; &lt;pre&gt; $ ls -l /usr/lib/.build-id/c1/e1977d6c15f173215ce21f017c50aa577bb50d lrwxrwxrwx. 1 root root 22 Jul 7 09:15 /usr/lib/.build-id/c1/e1977d6c15f173215ce21f017c50aa577bb50d -&gt; ../../../../usr/bin/ls &lt;/pre&gt; &lt;p&gt;&lt;code&gt;eu-unstrip&lt;/code&gt; also works on core files by using &lt;code&gt;--core=&lt;em&gt;FILENAME&lt;/em&gt;&lt;/code&gt; instead of the &lt;code&gt;-e&lt;/code&gt; option.&lt;/p&gt; &lt;h3&gt;Inspect the DWARF debugging information&lt;/h3&gt; &lt;p&gt;To inspect a program's DWARF debugging information, use &lt;code&gt;readelf -w&lt;/code&gt; (or the equivalent elfutils program, &lt;code&gt;eu-readelf&lt;/code&gt;) with the ELF file. If the file contains a build ID, &lt;code&gt;readelf&lt;/code&gt; will automatically find the separate debuginfo:&lt;/p&gt; &lt;pre&gt; $ readelf -w /usr/bin/ls | head -15 /usr/bin/ls: Found separate debug info file: /usr/lib/debug//usr/bin//ls-8.32-30.fc34.x86_64.debug /usr/lib/debug//usr/bin//ls-8.32-30.fc34.x86_64.debug: Found separate debug info file: /usr/lib/debug/usr/bin/../../.dwz/coreutils-8.32-30.fc34.x86_64 Contents of the .eh_frame section (loaded from /usr/bin/ls): 00000000 0000000000000014 00000000 CIE Version: 1 Augmentation: "zR" Code alignment factor: 1 Data alignment factor: -8 Return address column: 16 Augmentation data: 1b DW_CFA_def_cfa: r7 (rsp) ofs 8 &lt;/pre&gt; &lt;h3&gt;Was my program compiled with debuginfo?&lt;/h3&gt; &lt;p&gt;One question that comes up often on the GDB libera.chat IRC channel is whether a user has compiled their program or library with debugging information. There are several ways to check this, including inspecting &lt;code&gt;readelf&lt;/code&gt; and &lt;code&gt;objdump&lt;/code&gt; output, but I often find querying the DWARF compilation unit’s &lt;code&gt;producer&lt;/code&gt; attribute useful. This gives the full list of compile flags passed to the compiler:&lt;/p&gt; &lt;pre&gt; $ readelf -w hello | grep producer | head -1 DW_AT_producer : (indirect string, offset: 0x4b): GNU C17 11.2.1 20210728 (Red Hat 11.2.1-1) -mtune=generic -march=x86-64 -g &lt;/pre&gt; &lt;p&gt;The output tells us exactly what compiler was used and what flags were used to compile the object file. In this case (the "Hello, World!" example program), the program was compiled with GCC 11.2.1-1, using the &lt;code&gt;-g&lt;/code&gt; option to include debugging information. (The other &lt;code&gt;-m&lt;/code&gt; flags are automatically added by Fedora’s GCC configuration.)&lt;/p&gt; &lt;p&gt;As a bonus, this query can also answer the second most commonly asked question on the GDB IRC channel: Was your program built with optimization? Since the &lt;code&gt;DW_AT_producer&lt;/code&gt; string listed above does not contain any optimization flags (&lt;code&gt;-O&lt;em&gt;N&lt;/em&gt;&lt;/code&gt;), we know that this file was not compiled with any optimization.&lt;/p&gt; &lt;p&gt;For complex programs that contain many compilation units, it might be necessary to dump the output of &lt;code&gt;readelf&lt;/code&gt; to a file (or pipe it to &lt;code&gt;less&lt;/code&gt;) and search for the right compilation unit before looking at the &lt;code&gt;producer&lt;/code&gt; attribute.&lt;/p&gt; &lt;h2&gt;Next up in this series&lt;/h2&gt; &lt;p&gt;In this article, I have presented the very basic &lt;em&gt;what, when, where&lt;/em&gt;, and &lt;em&gt;how&lt;/em&gt; of debugging information. In the next article in this series, I will return to the GNU Debugger and discuss how to work with files of all kinds, including object and source files and shared libraries.&lt;/p&gt; &lt;p&gt;Do you have a suggestion or tip related to debugging information or a suggestion for a future topic about how to use GDB? Are you interested in a more in-depth article on DWARF? Leave a comment on this article and share your ideas or requests.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/01/10/gdb-developers-gnu-debugger-tutorial-part-2-all-about-debuginfo" title="The GDB developer’s GNU Debugger tutorial, Part 2: All about debuginfo"&gt;The GDB developer’s GNU Debugger tutorial, Part 2: All about debuginfo&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Keith Seitz</dc:creator><dc:date>2022-01-10T07:00:00Z</dc:date></entry><entry><title type="html">CodeReady Containers - Beginners guide to OpenShift Container Platform 4.9.10 with business automation tooling</title><link rel="alternate" href="http://www.schabell.org/2022/01/codeready-containers-beginners-guide-openshift-business-automation-tooling.html" /><author><name>Eric D. Schabell</name></author><id>http://www.schabell.org/2022/01/codeready-containers-beginners-guide-openshift-business-automation-tooling.html</id><updated>2022-01-10T06:00:00Z</updated><content type="html">Some time ago, back in December of 2020, I shared with you how to tooling on your developer machine using CodeReady Containers.  In this four part series you were familiarised with the OpenShift Container Platform, the business automation operators, and given a project to install the developer tooling needed to begin designing processes, rules, and much more. Recently, there were updates to the CodeReady Containers offering that gives you the latest OpenShift Container Platform 4.9.10 to run quite easily on your local developer machine. Time to update the various projects for leveraging to learn business automation tooling better known as the Red Hat Process Automation Manager and Red Hat Decision Manager. Along with these installation projects, you'll be able to use the free online workshops with the latests tooling. Let's take a look at the fastest way to get started, shall we? INSTALLING CODEREADY CONTAINERS Once upon a time in the not so distant past, I put together a project. This is no longer needed, just download  from the Red Hat Developer site and install following the provided documentation. Verify the correct installation with the following from the command line (terminal): $ crc version CodeReady Containers version: 1.37.0+3876d27d OpenShift version: 4.9.10 Once installed, you can then make use of the following projects to start exploring business automation developer tooling on your new OpenShift Container Platform. INSTALLING DECISION MANAGER The following project was setup to get you started with with just a few easy steps: 1. Ensure you have installed OpenShift with CodeReady Containers from  2. 3. Run 'init.sh' or 'init.bat' file. 'init.bat' must be run with Administrative privileges. Now log in to Red Hat Decision Manager to start developing containerized process automation projects (the address will be generated by OCP): * CodeReady Container example: https://insecure-rhdm-rhdmcentr-appdev-in-cloud.apps-crc.testing ( u:erics / p:redhatdm1! ) Not sure how to get started with Red Hat Decision Manager? Try one of these  to build a first project from scratch. INSTALLING PROCESS AUTOMATION MANAGER The following project was setup to get you started with the latest process automation developer tooling with just a few easy steps: 1. Ensure you have installed OpenShift with CodeReady Containers from  2. 3. Run 'init.sh' or 'init.bat' file. 'init.bat' must be run with Administrative privileges. Now log in to Red Hat Process Automation Manager to start developing containerized process automation projects (the address will be generated by OCP): * CodeReady Container example: https://insecure-rhpam-rhpamcentr-appdev-in-cloud.apps-crc.testing ( u:erics / p:redhatpam1! ) Not sure how to get started with Red Hat Process Automation Manager? Try one of these  to build a first project from scratch. WORKSHOPS AND MORE After installing the above tooling, you've been pointed to the freely available online workshops to start developing your first projects.  There are many more projects for you to explore without building them from scratch over on .  Explore all the projects with CodeReady Containers in the title and you should be well on your way to designing and executing business automation projects of your very own.</content><dc:creator>Eric D. Schabell</dc:creator></entry></feed>
